{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Language Model\n",
    "\n",
    "This is the \"working notebook\", with skeleton code to load and train your model, as well as run unit tests. See [rnnlm-instructions.ipynb](rnnlm-instructions.ipynb) for the main writeup.\n",
    "\n",
    "Run the cell below to import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danlee85/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm_test' from '/home/danlee85/w266/assignment/a3/lstm/rnnlm_test.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# Your code\n",
    "import rnnlm; reload(rnnlm)\n",
    "import rnnlm_test; reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) RNNLM Inputs and Parameters  \n",
    "\n",
    "### Questions for Part (a)\n",
    "You should use big-O notation when appropriate (i.e. computing $\\exp(\\mathbf{v})$ for a vector $\\mathbf{v}$ of length $n$ is $O(n)$ operations).  Assume for problems a(1-5) that:   \n",
    "> -- Cell is one layer,  \n",
    "> -- the embedding feature length and hidden-layer feature lengths are both H, and   \n",
    "> -- ignore at the moment batch and max_time dimensions.  \n",
    "\n",
    "1. Let $\\text{CellFunc}$ be a simple RNN cell (see async Section 5.8). Write the cell equation in terms of nonlinearities and matrix multiplication. How many parameters (matrix or vector elements) are there for this cell, in terms of `V` and `H`?\n",
    "<p>\n",
    "2. How many parameters are in the embedding layer? In the output layer? (By parameters, we mean total number of matrix elements across all train-able tensors. A $m \\times n$ matrix has $mn$ elements.)\n",
    "<p>\n",
    "3. How many calculations (floating point operations) are required to compute $\\hat{P}(w^{(i+1)})$ for a given *single* target word $w^{(i+1)}$, assuming $w^{(i)}$ given and $h^{(i-1)}$ already computed? How about for *all* target words?\n",
    "<p>\n",
    "4. How does your answer to 3. change if we approximate $\\hat{P}(w^{(i+1)})$ with a sampled softmax with $k$ samples? How about if we use a hierarchical softmax? (*Recall that hierarchical softmax makes a series of left/right decisions using a binary classifier $P_s(\\text{right}) = \\sigma(u_s \\cdot o^{(i)} + b_s)$ at each split $s$ in the tree.*)\n",
    "<p>\n",
    "5. If you have an LSTM with $H = 200$ and use sampled softmax with $k = 100$, what part of the network takes up the most computation time during training? (*Choose \"embedding layer\", \"recurrent layer\", or \"output layer\"*.)\n",
    "\n",
    "Note: for $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times l}$, computing the matrix product $AB$ takes $O(mnl)$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Implementing the RNNLM\n",
    "\n",
    "### Aside: Shapes Review\n",
    "\n",
    "Before we start, let's review our understanding of the shapes involved in this assignment and how they manifest themselves in the TF API.\n",
    "\n",
    "As in the [instructions](rnnlm-instructions.ipynb) notebook, $w$ is a matrix of wordids with shape batch_size x max_time.  Passing this through the embedding layer, we retrieve the word embedding for each, resulting in $x$ having shape batch_size x max_time x embedding_dim.  I find it useful to draw this out on a piece of paper.  When you do, you should end up with a rectangular prism with batch_size height, max_time width and embedding_dim depth.  Many tensors in this assignment share this shape (e.g. $o$, the output from the LSTM, which represents the hidden layer going into the softmax to make a prediction at every time step in every batch).\n",
    "\n",
    "![Three Dimensional Shape](common_shape.png)\n",
    "\n",
    "Since batch size and sentence length are only resolved when we run the graph, we construct the placeholder using \"None\" in the dimensions we don't know.  The .shape property renders these as ?s.  This should be familiar to you from batch size handling in earlier assignments, only now there are two dimensions of variable length.\n",
    "\n",
    "See the next cell for a concrete example (though in practice, we'd use a TensorFlow variable that we can train for the embeddings rather than a static array).  Notice how the shape of x_val matches the shape described earlier in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordid placeholder shape: (?, ?)\n",
      "x shape: (?, ?, 3)\n",
      "Embeddings shape: (2, 4, 3)\n",
      "Embeddings value:\n",
      " [[[2 2 2]\n",
      "  [3 3 3]\n",
      "  [2 2 2]\n",
      "  [3 3 3]]\n",
      "\n",
      " [[1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "wordid_ph = tf.placeholder(tf.int32, shape=[None, None])\n",
    "embedding_matrix = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "x = tf.nn.embedding_lookup(embedding_matrix, wordid_ph)\n",
    "\n",
    "print('wordid placeholder shape:', wordid_ph.shape)\n",
    "print('x shape:', x.shape)\n",
    "\n",
    "sess = tf.Session()\n",
    "# Two sentences, each with four words.\n",
    "wordids = [[1, 2, 1, 2], [0, 0, 0, 0]]\n",
    "x_val = sess.run(x, feed_dict={wordid_ph: wordids})\n",
    "print('Embeddings shape:', x_val.shape)  # 2 sentences, 4 words, \n",
    "print('Embeddings value:\\n', x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implmenting the RNNLM\n",
    "\n",
    "In order to better manage the model parameters, we'll implement our RNNLM in the `RNNLM` class in `rnnlm.py`. We've given you a skeleton of starter code for this, but the bulk of the implementation is left to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(rnnlm)\n",
    "\n",
    "TF_GRAPHDIR = \"/tmp/w266/a3_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will load your implementation, construct the graph, and write a logdir for TensorBoard. You can bring up TensorBoard with:\n",
    "```\n",
    "cd assignment/a3\n",
    "tensorboard --logdir /tmp/w266/a3_graph --port 6006\n",
    "```\n",
    "As usual, check http://localhost:6006/ and visit the \"Graphs\" tab to inspect your implementation. Remember, judicious use of `tf.name_scope()` and/or `tf.variable_scope()` will greatly improve the visualization, and make code easier to debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided a few unit tests below to verify some *very* basic properties of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_shapes_embed (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_output (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_recurrent (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_train (rnnlm_test.TestRNNLMTrain) ... ok\n",
      "test_shapes_sample (rnnlm_test.TestRNNLMSampler) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 2.084s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the error messages are intentionally somewhat spare, and that passing tests are no guarantee of model correctness! Your best chance of success is through careful coding and understanding of how the model works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Training your RNNLM (5 points)\n",
    "\n",
    "We'll give you data loader functions in **`utils.py`**. They work similarly to the loaders in the Week 5 notebook.\n",
    "\n",
    "Particularly, `utils.rnnlm_batch_generator` will return an iterator that yields minibatches in the correct format. Batches will be of size `[batch_size, max_time]`, and consecutive batches will line up along rows so that the final state $h^{\\text{final}}$ of one batch can be used as the initial state $h^{\\text{init}}$ for the next.\n",
    "\n",
    "For example, using a toy corpus:  \n",
    "*(Ignore the ugly formatter code.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Input words w:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "      <th>w_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little</td>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Target words y:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_corpus = \"<s> Mary had a little lamb . <s> The lamb was white as snow . <s>\"\n",
    "toy_corpus = np.array(toy_corpus.split())\n",
    "\n",
    "html = \"<h3>Input words w:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"w_%d\" % d for d in range(w.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(w, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))\n",
    "\n",
    "html = \"<h3>Target words y:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"y_%d\" % d for d in range(y.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(y, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data we feed to our model will be word indices, but the shape will be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the `run_epoch` function\n",
    "We've given you some starter code for logging progress; fill this in with actual call(s) to `session.run` with the appropriate arguments to run a training step. \n",
    "\n",
    "Be sure to handle the initial state properly at the beginning of an epoch, and remember to carry over the final state from each batch and use it as the initial state for the next.\n",
    "\n",
    "**Note:** we provide a `train=True` flag to enable train mode. If `train=False`, this function can also be used for scoring the dataset - see `score_dataset()` below.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "1.  Explain what this function does.  Be sure to include the role of `batch_iterator` and what's going on with `h` in the inner loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        feed_dict = {\n",
    "            lm.input_w_: w,\n",
    "            lm.target_y_: y,\n",
    "            lm.initial_h_: h,\n",
    "            lm.learning_rate_: learning_rate,\n",
    "            lm.use_dropout_: use_dropout\n",
    "        }\n",
    "        ops = [loss, lm.final_h_, train_op]        \n",
    "        #### YOUR CODE HERE ####\n",
    "        # session.run(...) the ops with the feed_dict constructed above.\n",
    "        # Ensure \"cost\" becomes the value of \"loss\".\n",
    "        # Hint: see \"ops\" for other variables that need updating in this loop.\n",
    "        \n",
    "\n",
    "        cost, h, none = session.run(ops, feed_dict = feed_dict)\n",
    "        \n",
    "        \n",
    "    \n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the cell below to verify your implementation of `run_epoch`, and to test your RNN on a (very simple) toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_toy_model (rnnlm_test.RunEpochTester) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 0]: seen 50 words at 45.8 wps, loss = 1.849\n",
      "[batch 201]: seen 10100 words at 4816.3 wps, loss = 0.541\n",
      "[batch 400]: seen 20050 words at 6469.9 wps, loss = 0.351\n",
      "[batch 598]: seen 29950 words at 7306.1 wps, loss = 0.274\n",
      "[batch 799]: seen 40000 words at 7836.9 wps, loss = 0.231\n",
      "[batch 998]: seen 49950 words at 8182.7 wps, loss = 0.200\n",
      "[batch 1197]: seen 59900 words at 8428.5 wps, loss = 0.180\n",
      "[batch 1397]: seen 69900 words at 8619.9 wps, loss = 0.165\n",
      "[batch 1596]: seen 79850 words at 8761.3 wps, loss = 0.153\n",
      "[batch 1797]: seen 89900 words at 8885.4 wps, loss = 0.142\n",
      "[batch 1997]: seen 99900 words at 8983.5 wps, loss = 0.135\n",
      "[batch 2195]: seen 109800 words at 9057.0 wps, loss = 0.128\n",
      "[batch 2395]: seen 119800 words at 9126.6 wps, loss = 0.122\n",
      "[batch 2592]: seen 129650 words at 9176.5 wps, loss = 0.117\n",
      "[batch 2791]: seen 139600 words at 9225.2 wps, loss = 0.113\n",
      "[batch 2991]: seen 149600 words at 9273.0 wps, loss = 0.109\n",
      "[batch 3189]: seen 159500 words at 9308.4 wps, loss = 0.106\n",
      "[batch 3391]: seen 169600 words at 9349.7 wps, loss = 0.103\n",
      "[batch 3592]: seen 179650 words at 9384.7 wps, loss = 0.100\n",
      "[batch 3794]: seen 189750 words at 9419.9 wps, loss = 0.098\n",
      "[batch 3992]: seen 199650 words at 9441.7 wps, loss = 0.096\n",
      "Train set: avg. loss: 0.000  (perplexity: 1.00)\n",
      "Test set: avg. loss: 0.020  (perplexity: 1.02)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 22.916s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as above, this is a *very* simple test case that does not guarantee model correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run Training\n",
    "\n",
    "We'll give you the outline of the training procedure, but you'll need to fill in a call to your `run_epoch` function. \n",
    "\n",
    "At the end of training, we use a `tf.train.Saver` to save a copy of the model to `/tmp/w266/a3_model/rnnlm_trained`. You'll want to load this from disk to work on later parts of the assignment; see **part (d)** for an example of how this is done.\n",
    "\n",
    "#### Tuning Hyperparameters\n",
    "With a sampled softmax loss, the default hyperparameters should train 5 epochs in around 15 minutes on a single-core GCE instance, and reach a training set perplexity between 120-140.\n",
    "\n",
    "However, it's possible to do significantly better. Try experimenting with multiple RNN layers (`num_layers` > 1) or a larger hidden state - though you may also need to adjust the learning rate and number of epochs for a larger model.\n",
    "\n",
    "You can also experiment with a larger vocabulary. This will look worse for perplexity, but will be a better model overall as it won't treat so many words as `<unk>`.\n",
    "\n",
    "#### Notes on Speed\n",
    "\n",
    "To speed things up, you may want to re-start your GCE instance with more CPUs. Using a 16-core machine will train *very* quickly if using a sampled softmax lost, almost as fast as a GPU. (Because of the sequential nature of the model, GPUs aren't actually much faster than CPUs for training and running RNNs.) The training code will print the words-per-second processed; with the default settings on a single core, you can expect around 8000 WPS, or up to more than 25000 WPS on a fast multi-core machine.\n",
    "\n",
    "You might also want to modify the code below to only run score_dataset at the very end, after all epochs are completed. This will speed things up significantly, since `score_dataset` uses the full softmax loss - and so often can take longer than a whole training epoch!\n",
    "\n",
    "#### Submitting your model\n",
    "You should submit your trained model along with the assignment. Do:\n",
    "```\n",
    "cp /tmp/w266/a3_model/rnnlm_trained* .\n",
    "git add rnnlm_trained*\n",
    "git commit -m \"Adding trained model.\"\n",
    "```\n",
    "Unless you train a very large model, these files should be < 50 MB and no problem for git to handle. If you do also train a large model, please only submit the smaller one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/danlee85/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Vocabulary: 10,000 types\n",
      "Loaded 57,340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45,872 sentences (924,077 tokens)\n",
      "Test set: 11,468 sentences (237,115 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "V = 10000\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 25\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=200, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=2)\n",
    "\n",
    "TF_SAVEDIR = \"/tmp/w266/a3_model\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[batch 16]: seen 42500 words at 4028.7 wps, loss = 6.765\n",
      "[batch 33]: seen 85000 words at 4062.5 wps, loss = 6.142\n",
      "[batch 50]: seen 127500 words at 4080.8 wps, loss = 5.766\n",
      "[batch 67]: seen 170000 words at 4090.1 wps, loss = 5.533\n",
      "[batch 84]: seen 212500 words at 4095.3 wps, loss = 5.375\n",
      "[batch 101]: seen 255000 words at 4095.9 wps, loss = 5.263\n",
      "[batch 118]: seen 297500 words at 4098.3 wps, loss = 5.169\n",
      "[batch 135]: seen 340000 words at 4100.5 wps, loss = 5.100\n",
      "[batch 152]: seen 382500 words at 4100.5 wps, loss = 5.038\n",
      "[batch 169]: seen 425000 words at 4099.2 wps, loss = 4.985\n",
      "[batch 186]: seen 467500 words at 4099.4 wps, loss = 4.937\n",
      "[batch 203]: seen 510000 words at 4100.3 wps, loss = 4.896\n",
      "[batch 220]: seen 552500 words at 4100.0 wps, loss = 4.856\n",
      "[batch 237]: seen 595000 words at 4100.3 wps, loss = 4.823\n",
      "[batch 254]: seen 637500 words at 4101.3 wps, loss = 4.793\n",
      "[batch 271]: seen 680000 words at 4102.8 wps, loss = 4.765\n",
      "[batch 288]: seen 722500 words at 4105.0 wps, loss = 4.739\n",
      "[batch 305]: seen 765000 words at 4099.7 wps, loss = 4.716\n",
      "[batch 321]: seen 805000 words at 4086.7 wps, loss = 4.694\n",
      "[batch 338]: seen 847500 words at 4086.3 wps, loss = 4.672\n",
      "[batch 355]: seen 890000 words at 4088.3 wps, loss = 4.652\n",
      "[batch 372]: seen 932500 words at 4088.3 wps, loss = 4.633\n",
      "[epoch 1] Completed in 0:03:57\n",
      "[epoch 1] Train set: avg. loss: 5.402  (perplexity: 221.95)\n",
      "[epoch 1] Test set: avg. loss: 5.443  (perplexity: 231.10)\n",
      "\n",
      "[epoch 2] Starting epoch 2\n",
      "[batch 16]: seen 42500 words at 4091.0 wps, loss = 4.223\n",
      "[batch 33]: seen 85000 words at 4087.9 wps, loss = 4.213\n",
      "[batch 50]: seen 127500 words at 4088.7 wps, loss = 4.195\n",
      "[batch 67]: seen 170000 words at 4085.1 wps, loss = 4.186\n",
      "[batch 84]: seen 212500 words at 4084.1 wps, loss = 4.175\n",
      "[batch 101]: seen 255000 words at 4079.9 wps, loss = 4.164\n",
      "[batch 118]: seen 297500 words at 4081.0 wps, loss = 4.162\n",
      "[batch 135]: seen 340000 words at 4080.7 wps, loss = 4.160\n",
      "[batch 152]: seen 382500 words at 4080.6 wps, loss = 4.150\n",
      "[batch 169]: seen 425000 words at 4080.1 wps, loss = 4.145\n",
      "[batch 186]: seen 467500 words at 4079.6 wps, loss = 4.140\n",
      "[batch 203]: seen 510000 words at 4078.1 wps, loss = 4.133\n",
      "[batch 220]: seen 552500 words at 4074.0 wps, loss = 4.129\n",
      "[batch 236]: seen 592500 words at 4053.0 wps, loss = 4.124\n",
      "[batch 252]: seen 632500 words at 4045.5 wps, loss = 4.117\n",
      "[batch 269]: seen 675000 words at 4047.6 wps, loss = 4.114\n",
      "[batch 286]: seen 717500 words at 4049.0 wps, loss = 4.109\n",
      "[batch 303]: seen 760000 words at 4050.9 wps, loss = 4.106\n",
      "[batch 320]: seen 802500 words at 4051.8 wps, loss = 4.102\n",
      "[batch 337]: seen 845000 words at 4053.0 wps, loss = 4.098\n",
      "[batch 354]: seen 887500 words at 4053.6 wps, loss = 4.095\n",
      "[batch 371]: seen 930000 words at 4054.9 wps, loss = 4.091\n",
      "[epoch 2] Completed in 0:03:59\n",
      "[epoch 2] Train set: avg. loss: 5.204  (perplexity: 181.99)\n",
      "[epoch 2] Test set: avg. loss: 5.259  (perplexity: 192.24)\n",
      "\n",
      "[epoch 3] Starting epoch 3\n",
      "[batch 16]: seen 42500 words at 4039.1 wps, loss = 3.985\n",
      "[batch 33]: seen 85000 words at 4038.0 wps, loss = 3.999\n",
      "[batch 50]: seen 127500 words at 4042.9 wps, loss = 3.991\n",
      "[batch 67]: seen 170000 words at 4053.5 wps, loss = 3.980\n",
      "[batch 84]: seen 212500 words at 4058.2 wps, loss = 3.972\n",
      "[batch 101]: seen 255000 words at 4060.6 wps, loss = 3.968\n",
      "[batch 118]: seen 297500 words at 4064.9 wps, loss = 3.966\n",
      "[batch 135]: seen 340000 words at 4058.1 wps, loss = 3.965\n",
      "[batch 151]: seen 380000 words at 4029.5 wps, loss = 3.960\n",
      "[batch 167]: seen 420000 words at 4022.4 wps, loss = 3.960\n",
      "[batch 184]: seen 462500 words at 4028.4 wps, loss = 3.955\n",
      "[batch 201]: seen 505000 words at 4032.7 wps, loss = 3.952\n",
      "[batch 218]: seen 547500 words at 4036.3 wps, loss = 3.951\n",
      "[batch 235]: seen 590000 words at 4039.8 wps, loss = 3.949\n",
      "[batch 252]: seen 632500 words at 4041.7 wps, loss = 3.946\n",
      "[batch 269]: seen 675000 words at 4044.3 wps, loss = 3.944\n",
      "[batch 286]: seen 717500 words at 4046.6 wps, loss = 3.941\n",
      "[batch 303]: seen 760000 words at 4049.3 wps, loss = 3.943\n",
      "[batch 320]: seen 802500 words at 4051.4 wps, loss = 3.940\n",
      "[batch 337]: seen 845000 words at 4053.3 wps, loss = 3.938\n",
      "[batch 354]: seen 887500 words at 4055.7 wps, loss = 3.936\n",
      "[batch 371]: seen 930000 words at 4058.1 wps, loss = 3.934\n",
      "[epoch 3] Completed in 0:03:58\n",
      "[epoch 3] Train set: avg. loss: 5.095  (perplexity: 163.25)\n",
      "[epoch 3] Test set: avg. loss: 5.164  (perplexity: 174.94)\n",
      "\n",
      "[epoch 4] Starting epoch 4\n",
      "[batch 16]: seen 42500 words at 4050.1 wps, loss = 3.910\n",
      "[batch 33]: seen 85000 words at 4063.0 wps, loss = 3.894\n",
      "[batch 50]: seen 127500 words at 4046.5 wps, loss = 3.886\n",
      "[batch 66]: seen 167500 words at 3991.3 wps, loss = 3.879\n",
      "[batch 82]: seen 207500 words at 3987.6 wps, loss = 3.875\n",
      "[batch 99]: seen 250000 words at 4006.5 wps, loss = 3.870\n",
      "[batch 116]: seen 292500 words at 4017.7 wps, loss = 3.868\n",
      "[batch 133]: seen 335000 words at 4027.4 wps, loss = 3.868\n",
      "[batch 150]: seen 377500 words at 4034.6 wps, loss = 3.865\n",
      "[batch 167]: seen 420000 words at 4039.5 wps, loss = 3.862\n",
      "[batch 184]: seen 462500 words at 4044.1 wps, loss = 3.861\n",
      "[batch 201]: seen 505000 words at 4046.2 wps, loss = 3.859\n",
      "[batch 218]: seen 547500 words at 4050.3 wps, loss = 3.858\n",
      "[batch 235]: seen 590000 words at 4052.4 wps, loss = 3.857\n",
      "[batch 252]: seen 632500 words at 4054.3 wps, loss = 3.856\n",
      "[batch 269]: seen 675000 words at 4055.9 wps, loss = 3.854\n",
      "[batch 286]: seen 717500 words at 4056.7 wps, loss = 3.853\n",
      "[batch 303]: seen 760000 words at 4058.1 wps, loss = 3.851\n",
      "[batch 320]: seen 802500 words at 4059.3 wps, loss = 3.850\n",
      "[batch 337]: seen 845000 words at 4060.5 wps, loss = 3.848\n",
      "[batch 354]: seen 887500 words at 4062.2 wps, loss = 3.846\n",
      "[batch 371]: seen 930000 words at 4063.3 wps, loss = 3.845\n",
      "[epoch 4] Completed in 0:03:58\n",
      "[epoch 4] Train set: avg. loss: 5.029  (perplexity: 152.74)\n",
      "[epoch 4] Test set: avg. loss: 5.110  (perplexity: 165.66)\n",
      "\n",
      "[epoch 5] Starting epoch 5\n",
      "[batch 16]: seen 42500 words at 4056.4 wps, loss = 3.852\n",
      "[batch 33]: seen 85000 words at 4068.3 wps, loss = 3.830\n",
      "[batch 50]: seen 127500 words at 4063.9 wps, loss = 3.827\n",
      "[batch 67]: seen 170000 words at 4063.8 wps, loss = 3.829\n",
      "[batch 84]: seen 212500 words at 4056.9 wps, loss = 3.823\n",
      "[batch 100]: seen 252500 words at 4047.3 wps, loss = 3.820\n",
      "[batch 117]: seen 295000 words at 4050.0 wps, loss = 3.815\n",
      "[batch 134]: seen 337500 words at 4052.3 wps, loss = 3.813\n",
      "[batch 151]: seen 380000 words at 4055.3 wps, loss = 3.810\n",
      "[batch 168]: seen 422500 words at 4057.2 wps, loss = 3.808\n",
      "[batch 185]: seen 465000 words at 4059.3 wps, loss = 3.806\n",
      "[batch 202]: seen 507500 words at 4059.9 wps, loss = 3.803\n",
      "[batch 219]: seen 550000 words at 4061.5 wps, loss = 3.802\n",
      "[batch 236]: seen 592500 words at 4062.7 wps, loss = 3.800\n",
      "[batch 253]: seen 635000 words at 4065.3 wps, loss = 3.797\n",
      "[batch 270]: seen 677500 words at 4066.8 wps, loss = 3.796\n",
      "[batch 287]: seen 720000 words at 4066.8 wps, loss = 3.795\n",
      "[batch 304]: seen 762500 words at 4067.6 wps, loss = 3.794\n",
      "[batch 321]: seen 805000 words at 4068.1 wps, loss = 3.792\n",
      "[batch 338]: seen 847500 words at 4068.6 wps, loss = 3.792\n",
      "[batch 355]: seen 890000 words at 4069.4 wps, loss = 3.791\n",
      "[batch 372]: seen 932500 words at 4069.3 wps, loss = 3.790\n",
      "[epoch 5] Completed in 0:03:58\n",
      "[epoch 5] Train set: avg. loss: 4.975  (perplexity: 144.75)\n",
      "[epoch 5] Test set: avg. loss: 5.069  (perplexity: 159.04)\n",
      "\n",
      "[epoch 6] Starting epoch 6\n",
      "[batch 16]: seen 42500 words at 4031.6 wps, loss = 3.764\n",
      "[batch 33]: seen 85000 words at 4037.0 wps, loss = 3.761\n",
      "[batch 50]: seen 127500 words at 4042.4 wps, loss = 3.760\n",
      "[batch 67]: seen 170000 words at 4043.4 wps, loss = 3.756\n",
      "[batch 84]: seen 212500 words at 4043.5 wps, loss = 3.755\n",
      "[batch 101]: seen 255000 words at 4044.6 wps, loss = 3.753\n",
      "[batch 118]: seen 297500 words at 4044.9 wps, loss = 3.753\n",
      "[batch 135]: seen 340000 words at 4044.7 wps, loss = 3.753\n",
      "[batch 152]: seen 382500 words at 4042.5 wps, loss = 3.751\n",
      "[batch 169]: seen 425000 words at 4043.2 wps, loss = 3.751\n",
      "[batch 186]: seen 467500 words at 4043.8 wps, loss = 3.750\n",
      "[batch 203]: seen 510000 words at 4043.7 wps, loss = 3.748\n",
      "[batch 220]: seen 552500 words at 4043.7 wps, loss = 3.747\n",
      "[batch 237]: seen 595000 words at 4042.4 wps, loss = 3.747\n",
      "[batch 254]: seen 637500 words at 4041.1 wps, loss = 3.745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 271]: seen 680000 words at 4040.0 wps, loss = 3.745\n",
      "[batch 288]: seen 722500 words at 4039.4 wps, loss = 3.745\n",
      "[batch 305]: seen 765000 words at 4039.0 wps, loss = 3.744\n",
      "[batch 322]: seen 807500 words at 4038.4 wps, loss = 3.744\n",
      "[batch 339]: seen 850000 words at 4038.2 wps, loss = 3.744\n",
      "[batch 356]: seen 892500 words at 4036.3 wps, loss = 3.744\n",
      "[batch 373]: seen 935000 words at 4036.2 wps, loss = 3.743\n",
      "[epoch 6] Completed in 0:04:00\n",
      "[epoch 6] Train set: avg. loss: 4.925  (perplexity: 137.65)\n",
      "[epoch 6] Test set: avg. loss: 5.031  (perplexity: 153.06)\n",
      "\n",
      "[epoch 7] Starting epoch 7\n",
      "[batch 16]: seen 42500 words at 4052.1 wps, loss = 3.724\n",
      "[batch 33]: seen 85000 words at 4057.2 wps, loss = 3.720\n",
      "[batch 50]: seen 127500 words at 4062.1 wps, loss = 3.721\n",
      "[batch 67]: seen 170000 words at 4070.3 wps, loss = 3.720\n",
      "[batch 84]: seen 212500 words at 4069.2 wps, loss = 3.718\n",
      "[batch 101]: seen 255000 words at 4068.5 wps, loss = 3.715\n",
      "[batch 118]: seen 297500 words at 4068.5 wps, loss = 3.714\n",
      "[batch 135]: seen 340000 words at 4069.8 wps, loss = 3.715\n",
      "[batch 152]: seen 382500 words at 4067.6 wps, loss = 3.713\n",
      "[batch 169]: seen 425000 words at 4066.4 wps, loss = 3.715\n",
      "[batch 186]: seen 467500 words at 4064.6 wps, loss = 3.714\n",
      "[batch 203]: seen 510000 words at 4065.5 wps, loss = 3.713\n",
      "[batch 220]: seen 552500 words at 4065.0 wps, loss = 3.713\n",
      "[batch 237]: seen 595000 words at 4063.7 wps, loss = 3.712\n",
      "[batch 254]: seen 637500 words at 4064.6 wps, loss = 3.712\n",
      "[batch 271]: seen 680000 words at 4064.2 wps, loss = 3.711\n",
      "[batch 288]: seen 722500 words at 4065.0 wps, loss = 3.711\n",
      "[batch 304]: seen 762500 words at 4052.0 wps, loss = 3.710\n",
      "[batch 320]: seen 802500 words at 4039.7 wps, loss = 3.709\n",
      "[batch 337]: seen 845000 words at 4041.1 wps, loss = 3.709\n",
      "[batch 354]: seen 887500 words at 4041.4 wps, loss = 3.709\n",
      "[batch 371]: seen 930000 words at 4042.1 wps, loss = 3.709\n",
      "[epoch 7] Completed in 0:03:59\n",
      "[epoch 7] Train set: avg. loss: 4.887  (perplexity: 132.58)\n",
      "[epoch 7] Test set: avg. loss: 5.004  (perplexity: 149.06)\n",
      "\n",
      "[epoch 8] Starting epoch 8\n",
      "[batch 16]: seen 42500 words at 4084.6 wps, loss = 3.709\n",
      "[batch 33]: seen 85000 words at 4095.2 wps, loss = 3.702\n",
      "[batch 50]: seen 127500 words at 4093.2 wps, loss = 3.703\n",
      "[batch 67]: seen 170000 words at 4096.2 wps, loss = 3.696\n",
      "[batch 84]: seen 212500 words at 4097.5 wps, loss = 3.693\n",
      "[batch 101]: seen 255000 words at 4098.7 wps, loss = 3.691\n",
      "[batch 118]: seen 297500 words at 4096.6 wps, loss = 3.691\n",
      "[batch 135]: seen 340000 words at 4094.6 wps, loss = 3.692\n",
      "[batch 152]: seen 382500 words at 4088.2 wps, loss = 3.690\n",
      "[batch 169]: seen 425000 words at 4086.8 wps, loss = 3.690\n",
      "[batch 186]: seen 467500 words at 4085.5 wps, loss = 3.689\n",
      "[batch 203]: seen 510000 words at 4086.1 wps, loss = 3.687\n",
      "[batch 219]: seen 550000 words at 4067.1 wps, loss = 3.686\n",
      "[batch 235]: seen 590000 words at 4048.6 wps, loss = 3.686\n",
      "[batch 252]: seen 632500 words at 4049.9 wps, loss = 3.686\n",
      "[batch 269]: seen 675000 words at 4051.5 wps, loss = 3.686\n",
      "[batch 286]: seen 717500 words at 4054.2 wps, loss = 3.686\n",
      "[batch 303]: seen 760000 words at 4054.2 wps, loss = 3.686\n",
      "[batch 320]: seen 802500 words at 4055.3 wps, loss = 3.685\n",
      "[batch 337]: seen 845000 words at 4056.9 wps, loss = 3.684\n",
      "[batch 354]: seen 887500 words at 4057.9 wps, loss = 3.684\n",
      "[batch 371]: seen 930000 words at 4058.9 wps, loss = 3.684\n",
      "[epoch 8] Completed in 0:03:58\n",
      "[epoch 8] Train set: avg. loss: 4.854  (perplexity: 128.29)\n",
      "[epoch 8] Test set: avg. loss: 4.983  (perplexity: 145.84)\n",
      "\n",
      "[epoch 9] Starting epoch 9\n",
      "[batch 16]: seen 42500 words at 4059.3 wps, loss = 3.672\n",
      "[batch 33]: seen 85000 words at 4060.3 wps, loss = 3.670\n",
      "[batch 50]: seen 127500 words at 4066.8 wps, loss = 3.674\n",
      "[batch 67]: seen 170000 words at 4068.7 wps, loss = 3.670\n",
      "[batch 84]: seen 212500 words at 4070.1 wps, loss = 3.667\n",
      "[batch 101]: seen 255000 words at 4071.3 wps, loss = 3.666\n",
      "[batch 118]: seen 297500 words at 4073.6 wps, loss = 3.665\n",
      "[batch 134]: seen 337500 words at 4041.5 wps, loss = 3.666\n",
      "[batch 150]: seen 377500 words at 4024.5 wps, loss = 3.666\n",
      "[batch 167]: seen 420000 words at 4029.5 wps, loss = 3.666\n",
      "[batch 184]: seen 462500 words at 4031.4 wps, loss = 3.665\n",
      "[batch 201]: seen 505000 words at 4035.0 wps, loss = 3.664\n",
      "[batch 218]: seen 547500 words at 4036.6 wps, loss = 3.663\n",
      "[batch 235]: seen 590000 words at 4037.8 wps, loss = 3.664\n",
      "[batch 252]: seen 632500 words at 4039.6 wps, loss = 3.663\n",
      "[batch 269]: seen 675000 words at 4040.1 wps, loss = 3.663\n",
      "[batch 286]: seen 717500 words at 4041.9 wps, loss = 3.662\n",
      "[batch 303]: seen 760000 words at 4043.8 wps, loss = 3.663\n",
      "[batch 320]: seen 802500 words at 4044.9 wps, loss = 3.662\n",
      "[batch 337]: seen 845000 words at 4047.0 wps, loss = 3.662\n",
      "[batch 354]: seen 887500 words at 4048.8 wps, loss = 3.661\n",
      "[batch 371]: seen 930000 words at 4049.3 wps, loss = 3.662\n",
      "[epoch 9] Completed in 0:03:59\n",
      "[epoch 9] Train set: avg. loss: 4.831  (perplexity: 125.30)\n",
      "[epoch 9] Test set: avg. loss: 4.968  (perplexity: 143.68)\n",
      "\n",
      "[epoch 10] Starting epoch 10\n",
      "[batch 16]: seen 42500 words at 4073.8 wps, loss = 3.653\n",
      "[batch 33]: seen 85000 words at 4078.9 wps, loss = 3.649\n",
      "[batch 49]: seen 125000 words at 4004.7 wps, loss = 3.649\n",
      "[batch 65]: seen 165000 words at 3973.1 wps, loss = 3.646\n",
      "[batch 82]: seen 207500 words at 3996.2 wps, loss = 3.647\n",
      "[batch 99]: seen 250000 words at 4011.8 wps, loss = 3.642\n",
      "[batch 116]: seen 292500 words at 4023.4 wps, loss = 3.642\n",
      "[batch 133]: seen 335000 words at 4030.7 wps, loss = 3.643\n",
      "[batch 150]: seen 377500 words at 4035.9 wps, loss = 3.642\n",
      "[batch 167]: seen 420000 words at 4041.0 wps, loss = 3.643\n",
      "[batch 184]: seen 462500 words at 4045.2 wps, loss = 3.643\n",
      "[batch 201]: seen 505000 words at 4048.6 wps, loss = 3.641\n",
      "[batch 218]: seen 547500 words at 4051.1 wps, loss = 3.641\n",
      "[batch 235]: seen 590000 words at 4052.7 wps, loss = 3.641\n",
      "[batch 252]: seen 632500 words at 4054.5 wps, loss = 3.640\n",
      "[batch 269]: seen 675000 words at 4057.0 wps, loss = 3.640\n",
      "[batch 286]: seen 717500 words at 4058.4 wps, loss = 3.640\n",
      "[batch 303]: seen 760000 words at 4059.9 wps, loss = 3.640\n",
      "[batch 320]: seen 802500 words at 4061.1 wps, loss = 3.639\n",
      "[batch 337]: seen 845000 words at 4062.2 wps, loss = 3.639\n",
      "[batch 354]: seen 887500 words at 4063.1 wps, loss = 3.639\n",
      "[batch 371]: seen 930000 words at 4064.4 wps, loss = 3.639\n",
      "[epoch 10] Completed in 0:03:58\n",
      "[epoch 10] Train set: avg. loss: 4.806  (perplexity: 122.29)\n",
      "[epoch 10] Test set: avg. loss: 4.954  (perplexity: 141.74)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.\n",
    "\n",
    "        run_epoch(lm, session, bi, train=True, verbose=True,tick_s=10, learning_rate=learning_rate)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print(\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Sampling Sentences (5 points)\n",
    "\n",
    "If you didn't already in **part (b)**, implement the `BuildSamplerGraph()` method in `rnnlm.py` See the function docstring for more information.\n",
    "\n",
    "#### Implement the `sample_step()` method below (5 points)\n",
    "This should access the Tensors you create in `BuildSamplerGraph()`. Given an input batch and initial states, it should return a vector of shape `[batch_size,1]` containing sampled indices for the next word of each batch sequence.\n",
    "\n",
    "Run the method using the provided code to generate 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "    #### YOUR CODE HERE ####\n",
    "    # Run sample ops\n",
    "\n",
    "    initial_h = session.run(lm.initial_h_, {lm.input_w_: input_w})\n",
    "    \n",
    "    feed_dict = {\n",
    "        lm.input_w_: input_w,\n",
    "        lm.initial_h_: initial_h,\n",
    "        }\n",
    "    \n",
    "    ops = [lm.final_h_, lm.pred_samples_]\n",
    "\n",
    "    final_h, samples = session.run(ops, feed_dict = feed_dict)\n",
    "\n",
    "    \n",
    "\n",
    "    #### END(YOUR CODE) ####\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a3_model/rnnlm_trained\n",
      "<s> no curve . <s> \n",
      "<s> he turned in <unk> agreement , as far to speak over addition to lock on your talent . <s> \n",
      "<s> <unk> , nuts <unk> rite of social craft <unk> doses of building , but a <unk> anglo-saxon george nixon of \n",
      "<s> and its mother's meat doesn't learned that he entered the <unk> might discover <unk> , however , served off him \n",
      "<s> she had a honest treatments . <s> \n",
      "<s> he knew it is <unk> cloth virginia . <s> \n",
      "<s> see first situation of valley . <s> \n",
      "<s> so treatment you'll become like a walnut milling ribs were <unk> and no <unk> as love questions , but <unk> \n",
      "<s> flat minutes . <s> \n",
      "<s> he nodded DGDGDGDG . <s> \n"
     ]
    }
   ],
   "source": [
    "# Same as above, but as a batch\n",
    "max_steps = 20\n",
    "num_samples = 10\n",
    "random_seed = 42\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "with lm.graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(random_seed)\n",
    "    \n",
    "    # Load the trained model\n",
    "    saver.restore(session, trained_filename)\n",
    "\n",
    "    # Make initial state for a batch with batch_size = num_samples\n",
    "    w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    # We'll take one step for each sequence on each iteration \n",
    "    for i in range(max_steps):\n",
    "        h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "        w = np.hstack((w,y))\n",
    "\n",
    "    # Print generated sentences\n",
    "    for row in w:\n",
    "        for i, word_id in enumerate(row):\n",
    "            print(vocab.id_to_word[word_id], end=\" \")\n",
    "            if (i != 0) and (word_id == vocab.START_ID):\n",
    "                break\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (e) Linguistic Properties (5 points)\n",
    "\n",
    "Now that we've trained our RNNLM, let's test a few properties of the model to see how well it learns linguistic phenomena. We'll do this with a scoring task: given two or more test sentences, our model should score the more plausible (or more correct) sentence with a higher log-probability.\n",
    "\n",
    "We'll define a scoring function to help us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "    \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "    padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq + [\"</s>\"], \n",
    "                                                             wordset=vocab.word_to_id))\n",
    "    w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "    y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.initial_h_:h,\n",
    "                 lm.dropout_keep_prob_: 1.0}\n",
    "    # Return log(P(seq)) = -1*loss\n",
    "    return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "    \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=lm.graph) as session:  \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "\n",
    "        if isinstance(inputs[0], str) or isinstance(inputs[0], bytes):\n",
    "            inputs = [inputs]\n",
    "\n",
    "        # Actually run scoring\n",
    "        results = []\n",
    "        for words in inputs:\n",
    "            score = score_seq(lm, session, words, vocab)\n",
    "            results.append((score, words))\n",
    "\n",
    "        # Sort if requested\n",
    "        if sort: results = sorted(results, reverse=True)\n",
    "\n",
    "        # Print results\n",
    "        for score, words in results:\n",
    "            print(\"\\\"{:s}\\\" : {:.02f}\".format(\" \".join(words), score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a3_model/rnnlm_trained\n",
      "\"once upon a time\" : -8.26\n",
      "\"the quick brown fox jumps over the lazy dog\" : -7.72\n"
     ]
    }
   ],
   "source": [
    "sents = [\"once upon a time\",\n",
    "         \"the quick brown fox jumps over the lazy dog\"]\n",
    "load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Number agreement\n",
    "\n",
    "Compare **\"the boy and the girl [are/is]\"**. Which is more plausible according to your model?\n",
    "\n",
    "If your model doesn't order them correctly (*this is OK*), why do you think that might be? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a3_model/rnnlm_trained\n",
      "\"the boy and the girl are\" : -5.95\n",
      "\"the boy and the girl is\" : -5.79\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "sents = [\"the boy and the girl are\",\n",
    "        \"the boy and the girl is\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Type/semantic agreement\n",
    "\n",
    "Compare:\n",
    "- **\"peanuts are my favorite kind of [nut/vegetable]\"**\n",
    "- **\"when I'm hungry I really prefer to [eat/drink]\"**\n",
    "\n",
    "Of each pair, which is more plausible according to your model?\n",
    "\n",
    "How would you expect a 3-gram language model to perform at this example? How about a 5-gram model? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a3_model/rnnlm_trained\n",
      "\"peanuts are my favorite kind of nut\" : -7.27\n",
      "\"peanuts are my favorite kind of vegetable\" : -6.99\n",
      "\"when I'm hungry I really prefer to eat\" : -7.47\n",
      "\"when I'm hungry I really prefer to drink\" : -7.65\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "sents = [\"peanuts are my favorite kind of nut\",\n",
    "         \"peanuts are my favorite kind of vegetable\",\n",
    "         \"when I'm hungry I really prefer to eat\",\n",
    "         \"when I'm hungry I really prefer to drink\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adjective ordering (just for fun)\n",
    "\n",
    "Let's repeat the exercise from Week 2:\n",
    "\n",
    "![Adjective Order](adjective_order.jpg)\n",
    "*source: https://twitter.com/MattAndersonBBC/status/772002757222002688?lang=en*\n",
    "\n",
    "We'll consider a toy example (literally), and consider all possible adjective permutations.\n",
    "\n",
    "Note that this is somewhat sensitive to training, and even a good language model might not get it all correct. Why might the NN fail, if the trigram model from Week 2 was able to solve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a3_model/rnnlm_trained\n",
      "\"I have lots of green plastic square toys\" : -8.04\n",
      "\"I have lots of plastic green square toys\" : -8.08\n",
      "\"I have lots of plastic square green toys\" : -8.13\n",
      "\"I have lots of green square plastic toys\" : -8.14\n",
      "\"I have lots of square plastic green toys\" : -8.26\n",
      "\"I have lots of square green plastic toys\" : -8.29\n"
     ]
    }
   ],
   "source": [
    "prefix = \"I have lots of\".split()\n",
    "noun = \"toys\"\n",
    "adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "inputs = []\n",
    "for adjs in itertools.permutations(adjectives):\n",
    "    words = prefix + list(adjs) + [noun]\n",
    "    inputs.append(words)\n",
    "    \n",
    "load_and_score(inputs, sort=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
