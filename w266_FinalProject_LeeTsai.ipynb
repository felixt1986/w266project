{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dropbox\n",
    "import urllib.request, json \n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "import datetime as DT\n",
    "import ast\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import csv\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary\n",
    "\n",
    "########################### DLEE IMPORTS ##################################\n",
    "\n",
    "# Import pandas and tensorflow\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.10\"))\n",
    "\n",
    "# Import model\n",
    "import models\n",
    "\n",
    "# Import misc\n",
    "from importlib import reload\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "import itertools\n",
    "from w266_common import patched_numpy_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Download the pre-processed tweets from dropbox\n",
    "\n",
    "access_token = 'p3Ze8FpdRTAAAAAAAAAADb9hCQ8aXXYU3A3gGM1HEXyeMrs8KI2SrA71KDmmCXte'\n",
    "dbx = dropbox.Dropbox(access_token)\n",
    "metadata, res = dbx.files_download(path=\"/Inputdata2.txt\")\n",
    "\n",
    "\n",
    "access_token2 = 'm_fKi8mWZ0AAAAAAAAAADlfzTm37f1y45k92Xpiw1B6mjT3dQqzNvKgpIUSR17uq'\n",
    "dbx2 = dropbox.Dropbox(access_token2)\n",
    "\n",
    "#Write the combined file into the drive\n",
    "with open(\"InputX.txt\", \"wb\") as f:\n",
    "    metadata, res = dbx.files_download(path=\"/Inputdata2.txt\")\n",
    "    metadata2, res2 = dbx2.files_download(path=\"/Inputdata.txt\")\n",
    "    f.write(res.content)\n",
    "    f.write(res2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Translate date of the tweet to quarter reference\n",
    "def Quarterlookup(date):\n",
    "    if date>=DT.datetime(2018,10,1):\n",
    "        return '18Q4'\n",
    "    elif date>=DT.datetime(2018,7,1):\n",
    "        return '18Q3'\n",
    "    elif date>=DT.datetime(2018,4,1):\n",
    "        return '18Q2'\n",
    "    elif date>=DT.datetime(2018,1,1):\n",
    "        return '18Q1'\n",
    "    elif date>=DT.datetime(2017,10,1):\n",
    "        return '17Q4'\n",
    "    elif date>=DT.datetime(2017,7,1):\n",
    "        return '17Q3'\n",
    "    elif date>=DT.datetime(2017,4,1):\n",
    "        return '17Q2'\n",
    "    elif date>=DT.datetime(2017,1,1):\n",
    "        return '17Q1'\n",
    "    elif date>=DT.datetime(2016,10,1):\n",
    "        return '16Q4'\n",
    "    elif date>=DT.datetime(2016,7,1):\n",
    "        return '16Q3'\n",
    "    elif date>=DT.datetime(2016,4,1):\n",
    "        return '16Q2'\n",
    "    elif date>=DT.datetime(2016,1,1):\n",
    "        return '16Q1'\n",
    "    elif date>=DT.datetime(2015,10,1):\n",
    "        return '15Q4'\n",
    "    elif date>=DT.datetime(2015,7,1):\n",
    "        return '15Q3'\n",
    "    elif date>=DT.datetime(2015,4,1):\n",
    "        return '15Q2'\n",
    "    else:\n",
    "        return '15Q1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reading the Input X file and process it\n",
    "d=defaultdict(list)\n",
    "\n",
    "with open('InputX.txt') as f:\n",
    "    for line in f:\n",
    "        count=0\n",
    "        temp =-1\n",
    "        k,v=line.split(':',1)\n",
    "        while True:\n",
    "            \n",
    "            oldtemp=temp\n",
    "            temp=v.find(']]',temp+1)\n",
    "            if count==0:\n",
    "                d[k].append(v[oldtemp+2:temp+2]) \n",
    "            else:\n",
    "                d[k].append(v[oldtemp+4:temp+2])\n",
    "           \n",
    "             \n",
    "            count+=1\n",
    "            if temp == -1: break\n",
    "#             if count == 500: break  # initially limit for time sake\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read from the vocab file pre prepared\n",
    "with open('vocab.csv', 'r') as f:\n",
    "    reader = csv.reader(f,delimiter=';')\n",
    "    allword = set(list(reader)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5,681\n"
     ]
    }
   ],
   "source": [
    "allword=sorted(allword)\n",
    "vocab = vocabulary.Vocabulary(allword, size=None)  # size=None means unlimited\n",
    "print(\"Vocabulary size: {:,}\".format(vocab.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_np_array(example_ids, max_len=50, pad_id=0):\n",
    "\n",
    "    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\n",
    "    ns = np.zeros([len(example_ids)], dtype=np.int32)\n",
    "    cpy_len = min(len(example_ids), max_len)\n",
    "\n",
    "    arr = example_ids[:cpy_len]\n",
    "    ns = cpy_len\n",
    "    return arr, ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary to collect the tweet detail and the length of each tweet\n",
    "InputX=defaultdict(dict)\n",
    "InputN=defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set of all the stocks of interest\n",
    "instock =['CVSHealth', 'RiteAid', \"Conn's\", \"Casey's\", 'Kroger','WeisMarkets', 'HavertyFurniture', \"Kirkland's\", 'Pier1Imports', 'BuildersFirstSource', 'HuttigBuildingProducts', 'LumberLiquidators', 'TransWorldEntertainment', 'MarineMax', 'Medifast', 'SallyBeauty',\"Sotheby's\", 'StarGasPartners', 'SuburbanPropanePartners', 'TitanMachinery', 'TractorSupply', 'TravelCentersofAmerica', 'Big5SportingGoods', \"Dick'sSportingGoods\", 'HibbettSports', 'Build-A-BearWorkshop']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Converting the string of data into dictionary\n",
    "\n",
    "\n",
    "for key,value in d.items():\n",
    "    Inputx=defaultdict(list)\n",
    "    Inputn=defaultdict(list)\n",
    "    if key in instock:\n",
    "        for value2 in value:\n",
    "        \n",
    "            temp=value2.find('), ')\n",
    "            date=value2[14:temp+3]\n",
    "            try:\n",
    "                year=int(date[1:5])\n",
    "                month=int(date[date.find(',')+2:date.find(',',date.find(',')+1)])\n",
    "                day=int(date[date.find(')')-2:date.find(')')])\n",
    "            \n",
    "                sentence, length = pad_np_array (vocab.words_to_ids(ast.literal_eval(value2[temp+3:-1])))\n",
    "            \n",
    "                Inputx[Quarterlookup(DT.datetime(year,month,day))].append(sentence)\n",
    "                Inputn[Quarterlookup(DT.datetime(year,month,day))].append(length)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    InputX[key]=OrderedDict(sorted(Inputx.items()))\n",
    "    InputN[key]=OrderedDict(sorted(Inputn.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CVSHealth': 'CVS', 'RiteAid': 'RAD', \"Conn's\": 'CONN', \"Casey's\": 'CASY', 'Kroger': 'KR', 'WeisMarkets': 'WMK', 'HavertyFurniture': 'HVT', \"Kirkland's\": 'KIRK', 'Pier1Imports': 'PIR', 'BuildersFirstSource': 'BLDR', 'HuttigBuildingProducts': 'HBP', 'LumberLiquidators': 'LL', 'TransWorldEntertainment': 'TWMC', 'MarineMax': 'HZO', 'Medifast': 'MED', 'SallyBeauty': 'SBH', \"Sotheby's\": 'BID', 'StarGasPartners': 'SGU', 'SuburbanPropanePartners': 'SPH', 'TitanMachinery': 'TITN', 'TractorSupply': 'TSCO', 'TravelCentersofAmerica': 'TA', 'Big5SportingGoods': 'BGFV', \"Dick'sSportingGoods\": 'DKS', 'HibbettSports': 'HIBB', 'Build-A-BearWorkshop': 'BBW'}\n"
     ]
    }
   ],
   "source": [
    "##List of stockname and relevant stock code\n",
    "l = [(\"CVSHealth\", \"CVS\"),\n",
    "(\"RiteAid\", \"RAD\"),\n",
    "(\"Conn's\", \"CONN\"),\n",
    "(\"Casey's\", \"CASY\"),\n",
    "(\"Kroger\", \"KR\"),\n",
    "(\"WeisMarkets\", \"WMK\"),\n",
    "(\"HavertyFurniture\", \"HVT\"),\n",
    "(\"Kirkland's\", \"KIRK\"),\n",
    "(\"Pier1Imports\",\"PIR\"),\n",
    "(\"BuildersFirstSource\",\"BLDR\"),\n",
    "(\"HuttigBuildingProducts\", \"HBP\"),\n",
    "(\"LumberLiquidators\", \"LL\"),\n",
    "(\"TransWorldEntertainment\", \"TWMC\"),\n",
    "(\"MarineMax\", \"HZO\"),\n",
    "(\"Medifast\", \"MED\"),\n",
    "(\"SallyBeauty\", \"SBH\"),\n",
    "(\"Sotheby's\", \"BID\"),\n",
    "(\"StarGasPartners\", \"SGU\"),\n",
    "(\"SuburbanPropanePartners\", \"SPH\"),\n",
    "(\"TitanMachinery\", \"TITN\"),\n",
    "(\"TractorSupply\",\"TSCO\"),\n",
    "(\"TravelCentersofAmerica\", \"TA\"),\n",
    "(\"Big5SportingGoods\", \"BGFV\"),\n",
    "(\"Dick'sSportingGoods\", \"DKS\"),\n",
    "(\"HibbettSports\", \"HIBB\"),\n",
    "(\"Build-A-BearWorkshop\", \"BBW\")]\n",
    "     \n",
    "all_stock = {}\n",
    "[all_stock.update({k:v}) for k,v in l]\n",
    "print(all_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Read the quarterly return detail from edgaronline and convert into library\n",
    "top=\"http://datafied.api.edgar-online.com/v2/corefinancials/qtr?primarysymbols=\"\n",
    "bottom=\"&appkey=asc97xrhkyu4959aptu76zxj\"\n",
    "Outputy =defaultdict(dict)\n",
    "for stockname,stock in all_stock.items():\n",
    "    check=top+stock+bottom\n",
    "    with urllib.request.urlopen(check) as url:\n",
    "        data = json.loads(url.read().decode())\n",
    "    count1=0\n",
    "    count2=0\n",
    "    end_date=[]\n",
    "    revenue_change=[]\n",
    "    last_rev=0\n",
    "\n",
    "    for key, value in data.items():\n",
    "        \n",
    "        for key2, value2 in value.items():          \n",
    "        \n",
    "            if count1!=0:  \n",
    "                for i in reversed(value2):\n",
    "                  \n",
    "                    for j in i[\"values\"]:\n",
    "                     \n",
    "                        if count2!=0:   \n",
    "                            if j[\"field\"] ==\"periodenddate\":\n",
    "                                end_date.append(Quarterlookup(DT.datetime.strptime(j['value'], '%m/%d/%Y')))\n",
    "                       \n",
    "                            ##Convert revenue into the y_label of 0 for negative growth and 1 for positive growth\n",
    "                            if j[\"field\"] ==\"totalrevenue\":\n",
    "                                revenue_cal =round(float(j['value'])/float(last_rev) - 1,3)\n",
    "                                if revenue_cal>0:\n",
    "                                    revenue_change.append(1)\n",
    "                                else:\n",
    "                                    revenue_change.append(0)\n",
    "                        else:\n",
    "                            \n",
    "                            if j[\"field\"] ==\"totalrevenue\":      \n",
    "                                last_rev=j['value']\n",
    "                              \n",
    "                    count2+=1\n",
    "            count1+=1\n",
    "            \n",
    "    ##Output will be a dictionary with stockname\n",
    "    Outputy[stockname][\"period\"]=end_date\n",
    "    Outputy[stockname][\"revenuechange\"]=revenue_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Outputy.txt', 'w') as f:\n",
    "    print(Outputy, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_n = []\n",
    "train_y = []\n",
    "test_x = []\n",
    "test_n = []\n",
    "test_y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter=['15Q1','15Q2','15Q3','15Q4','16Q1','16Q2','16Q3','16Q4','17Q1','17Q2','17Q3','17Q4','18Q1','18Q2','18Q3']\n",
    "\n",
    "for i in instock:\n",
    "    for j in range(len(quarter)):\n",
    "        try:\n",
    "            if quarter[j]=='18Q3':\n",
    "                test_x.append(InputX [i][quarter[j]])\n",
    "                test_n.append(InputN [i][quarter[j]])\n",
    "                length=len(InputN [i][quarter[j]])\n",
    "                test_y.append([Outputy [i]['revenuechange'][j]]*length)                \n",
    "               \n",
    "            else:\n",
    "                train_x.append(InputX [i][quarter[j]])\n",
    "                train_n.append(InputN [i][quarter[j]])\n",
    "                length=len(InputN [i][quarter[j]])           \n",
    "                train_y.append([Outputy [i]['revenuechange'][j]]*length)\n",
    "      \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL BAG OF WORDS MODEL #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23021\n",
      "23021\n",
      "23021\n",
      "34\n",
      "34\n",
      "23810\n",
      "23810\n",
      "23810\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x[0]))\n",
    "print(len(train_n[0]))\n",
    "print(len(train_y[0]))\n",
    "print(len(train_x[0][0]))\n",
    "print(train_n[0][0])\n",
    "\n",
    "print(len(test_x[0]))\n",
    "print(len(test_n[0]))\n",
    "print(len(test_y[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_init = train_x\n",
    "train_n_init = train_n\n",
    "train_y_init = train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_np_array2(example_ids, max_len=50, pad_id=0):\n",
    "\n",
    "    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\n",
    "    ns = np.zeros([len(example_ids)], dtype=np.int32)\n",
    "    for i, ids in enumerate(example_ids):\n",
    "        cpy_len = min(len(ids), max_len)\n",
    "        arr[i,:cpy_len] = ids[:cpy_len]\n",
    "        ns[i] = cpy_len\n",
    "    return arr, ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "[1946, 3275, 748, 4639, 2, 3399, 178, 2, 199, 1088, 987, 5287, 3522, 2, 1950, 2, 1951, 2, 4925, 2, 5585, 2, 750, 2, 1875, 2, 1876, 2, 907, 2, 5583, 2, 5584, 2, 3057, 2, 749, 2, 3058, 2, 2031, 2, 1029, 2, 1028, 2, 1497, 3663]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x_init[3][4]))\n",
    "print(train_x_init[3][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pad_np_array2(train_x_init[0], max_len = 50, pad_id=0)\n",
    "train_n = np.array(train_n_init[0])\n",
    "train_y = np.array(train_y_init[0])\n",
    "\n",
    "test_x = pad_np_array2(train_x_init[1], max_len = 50, pad_id=0)\n",
    "test_n = np.array(train_n_init[1])\n",
    "test_y = np.array(train_y_init[1])\n",
    "\n",
    "dev_x = pad_np_array2(train_x_init[2], max_len = 50, pad_id=0)\n",
    "dev_n = np.array(train_n_init[2])\n",
    "dev_y = np.array(train_y_init[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "23021\n",
      "23021\n",
      "(array([[   2, 5095, 2526, ...,    0,    0,    0],\n",
      "       [5006, 2796, 1790, ..., 3730,    0,    0],\n",
      "       [1946, 3275,  748, ..., 3663,    0,    0],\n",
      "       ...,\n",
      "       [   2, 4696,    2, ...,    0,    0,    0],\n",
      "       [2678,    2, 5006, ..., 1780,  663, 5678],\n",
      "       [ 454, 5605, 4241, ...,    0,    0,    0]], dtype=int32), array([34, 48, 48, ..., 39, 50, 25], dtype=int32))\n",
      "[34 48 48 ... 39 50 25]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[[   2 5095 2526 ...    0    0    0]\n",
      " [5006 2796 1790 ... 3730    0    0]\n",
      " [1946 3275  748 ... 3663    0    0]\n",
      " ...\n",
      " [   2 4696    2 ...    0    0    0]\n",
      " [2678    2 5006 ... 1780  663 5678]\n",
      " [ 454 5605 4241 ...    0    0    0]]\n",
      "[34 48 48 ... 39 50 25]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x))\n",
    "print(len(train_n))\n",
    "print(len(train_y))\n",
    "\n",
    "print(train_x)\n",
    "print(train_n)\n",
    "print(train_y)\n",
    "\n",
    "print(train_x[0])\n",
    "print(train_x[1])\n",
    "# print(train_x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x[0])\n",
    "dev_x = np.array(dev_x[0])\n",
    "test_x = np.array(test_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23021\n",
      "50\n",
      "23693\n",
      "50\n",
      "23568\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x))\n",
    "print(len(train_x[0]))\n",
    "print(len(dev_x))\n",
    "print(len(dev_x[0]))\n",
    "print(len(test_x))\n",
    "print(len(test_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 5095 2526 ...    0    0    0]\n",
      " [5006 2796 1790 ... 3730    0    0]\n",
      " [1946 3275  748 ... 3663    0    0]\n",
      " ...\n",
      " [   2 4696    2 ...    0    0    0]\n",
      " [2678    2 5006 ... 1780  663 5678]\n",
      " [ 454 5605 4241 ...    0    0    0]]\n",
      "[   2 5095 2526    2 3148    2  546    2 5094    2 5584    2  685    2\n",
      " 3058    2 5585    2 4898    2 1267    2 3058 3437 3148 2386    2   64\n",
      " 2612    2 5321    2 2464 5678    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_x)\n",
    "print(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23021\n",
      "23021\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(len(train_n))\n",
    "print(len(train_y))\n",
    "\n",
    "print(type(train_x))\n",
    "print(type(train_n))\n",
    "print(type(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  625 examples, moving-average loss 0.54\n",
      "1,250 examples, moving-average loss 0.35\n",
      "1,875 examples, moving-average loss 0.27\n",
      "2,500 examples, moving-average loss 0.23\n",
      "3,125 examples, moving-average loss 0.20\n",
      "3,750 examples, moving-average loss 0.17\n",
      "4,375 examples, moving-average loss 0.15\n",
      "5,000 examples, moving-average loss 0.13\n",
      "5,625 examples, moving-average loss 0.12\n",
      "6,250 examples, moving-average loss 0.11\n",
      "6,875 examples, moving-average loss 0.10\n",
      "7,500 examples, moving-average loss 0.09\n",
      "8,125 examples, moving-average loss 0.08\n",
      "8,750 examples, moving-average loss 0.07\n",
      "9,375 examples, moving-average loss 0.07\n",
      "10,000 examples, moving-average loss 0.06\n",
      "10,625 examples, moving-average loss 0.06\n",
      "11,250 examples, moving-average loss 0.06\n",
      "11,875 examples, moving-average loss 0.05\n",
      "12,500 examples, moving-average loss 0.05\n",
      "13,125 examples, moving-average loss 0.05\n",
      "13,750 examples, moving-average loss 0.05\n",
      "14,375 examples, moving-average loss 0.05\n",
      "15,000 examples, moving-average loss 0.04\n",
      "15,625 examples, moving-average loss 0.04\n",
      "16,250 examples, moving-average loss 0.04\n",
      "16,875 examples, moving-average loss 0.04\n",
      "17,500 examples, moving-average loss 0.04\n",
      "18,125 examples, moving-average loss 0.04\n",
      "18,750 examples, moving-average loss 0.04\n",
      "19,375 examples, moving-average loss 0.04\n",
      "20,000 examples, moving-average loss 0.04\n",
      "20,625 examples, moving-average loss 0.04\n",
      "21,250 examples, moving-average loss 0.04\n",
      "21,875 examples, moving-average loss 0.04\n",
      "22,500 examples, moving-average loss 0.04\n",
      "Completed one epoch in 0:00:04\n"
     ]
    }
   ],
   "source": [
    "reload(models)\n",
    "\n",
    "x, ns, y = train_x, train_n, train_y\n",
    "batch_size = 25\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=vocab.size, embed_dim=50, hidden_dims=[25], num_classes=2,\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "model_fn = models.classifier_model_fn\n",
    "\n",
    "total_batches = 0\n",
    "total_examples = 0\n",
    "total_loss = 0\n",
    "loss_ema = np.log(2)  # track exponential-moving-average of loss\n",
    "ema_decay = np.exp(-1/10)  # decay parameter for moving average = np.exp(-1/history_length)\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    ##\n",
    "    # Construct the graph here. No session.run calls - just wiring up Tensors.\n",
    "    ##\n",
    "    # Add placeholders so we can feed in data.\n",
    "    x_ph_  = tf.placeholder(tf.int32, shape=[None, x.shape[1]])  # [batch_size, max_len]\n",
    "    ns_ph_ = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    y_ph_  = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    \n",
    "    # Construct the graph using model_fn\n",
    "    features = {\"ids\": x_ph_, \"ns\": ns_ph_}  # note that values are Tensors\n",
    "    estimator_spec = model_fn(features, labels=y_ph_, mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                              params=model_params)\n",
    "    loss_     = estimator_spec.loss\n",
    "    train_op_ = estimator_spec.train_op\n",
    "    \n",
    "    ##\n",
    "    # Done constructing the graph, now we can make session.run calls.\n",
    "    ##\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Run a single epoch\n",
    "    t0 = time.time()\n",
    "    for (bx, bns, by) in utils.multi_batch_generator(batch_size, x, ns, y):\n",
    "        # feed NumPy arrays into the placeholder Tensors\n",
    "        feed_dict = {x_ph_: bx, ns_ph_: bns, y_ph_: by}\n",
    "        batch_loss, _ = sess.run([loss_, train_op_], feed_dict=feed_dict)\n",
    "        \n",
    "        # Compute some statistics\n",
    "        total_batches += 1\n",
    "        total_examples += len(bx)\n",
    "        total_loss += batch_loss * len(bx)  # re-scale, since batch loss is mean\n",
    "        # Compute moving average to smooth out noisy per-batch loss\n",
    "        loss_ema = ema_decay * loss_ema + (1 - ema_decay) * batch_loss\n",
    "        \n",
    "        if (total_batches % 25 == 0):\n",
    "            print(\"{:5,} examples, moving-average loss {:.2f}\".format(total_examples, \n",
    "                                                                      loss_ema))    \n",
    "    print(\"Completed one epoch in {:s}\".format(utils.pretty_timedelta(since=t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (5,681 words) written to '/tmp/tf_bow_sst_20181202-0353/metadata.tsv'\n",
      "Projector config written to /tmp/tf_bow_sst_20181202-0353/projector_config.pbtxt\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tf_bow_sst_20181202-0353', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff53e4cc550>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    tensorboard --logdir='/tmp/tf_bow_sst_20181202-0353' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "import models; reload(models)\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=vocab.size, embed_dim=50, hidden_dims=[25], num_classes=2,\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "\n",
    "checkpoint_dir = \"/tmp/tf_bow_sst_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "# Write vocabulary to file, so TensorBoard can label embeddings.\n",
    "# creates checkpoint_dir/projector_config.pbtxt and checkpoint_dir/metadata.tsv\n",
    "vocab.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=models.classifier_model_fn, \n",
    "                               params=model_params,\n",
    "                               model_dir=checkpoint_dir)\n",
    "print(\"\")\n",
    "print(\"To view training (once it starts), run:\\n\")\n",
    "print(\"    tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "print(\"\\nThen in your browser, open: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tf_bow_sst_20181202-0353/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1935925, step = 1\n",
      "INFO:tensorflow:global_step/sec: 212.56\n",
      "INFO:tensorflow:loss = 0.29453677, step = 101 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 240.953\n",
      "INFO:tensorflow:loss = 0.17239937, step = 201 (0.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.8\n",
      "INFO:tensorflow:loss = 0.10990701, step = 301 (0.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.663\n",
      "INFO:tensorflow:loss = 0.077133864, step = 401 (0.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.624\n",
      "INFO:tensorflow:loss = 0.060306218, step = 501 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.241\n",
      "INFO:tensorflow:loss = 0.049825072, step = 601 (0.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.695\n",
      "INFO:tensorflow:loss = 0.043526158, step = 701 (0.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.046\n",
      "INFO:tensorflow:loss = 0.04011548, step = 801 (0.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.881\n",
      "INFO:tensorflow:loss = 0.03886269, step = 901 (0.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.813\n",
      "INFO:tensorflow:loss = 0.038373705, step = 1001 (0.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 232.38\n",
      "INFO:tensorflow:loss = 0.036055386, step = 1101 (0.429 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.195\n",
      "INFO:tensorflow:loss = 0.035244055, step = 1201 (0.434 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.889\n",
      "INFO:tensorflow:loss = 0.03401329, step = 1301 (0.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.703\n",
      "INFO:tensorflow:loss = 0.034464356, step = 1401 (0.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.921\n",
      "INFO:tensorflow:loss = 0.035207253, step = 1501 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.166\n",
      "INFO:tensorflow:loss = 0.035351075, step = 1601 (0.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.006\n",
      "INFO:tensorflow:loss = 0.0342484, step = 1701 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.857\n",
      "INFO:tensorflow:loss = 0.035408, step = 1801 (0.417 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1842 into /tmp/tf_bow_sst_20181202-0353/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.032334566.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-02-03:54:08\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181202-0353/model.ckpt-1842\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-02-03:54:11\n",
      "INFO:tensorflow:Saving dict for global step 1842: accuracy = 1.0, cross_entropy_loss = 0.0053163907, global_step = 1842, loss = 0.03449681\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1842: /tmp/tf_bow_sst_20181202-0353/model.ckpt-1842\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181202-0353/model.ckpt-1842\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1842 into /tmp/tf_bow_sst_20181202-0353/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.034037024, step = 1843\n",
      "INFO:tensorflow:global_step/sec: 215.708\n",
      "INFO:tensorflow:loss = 0.034384474, step = 1943 (0.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.016\n",
      "INFO:tensorflow:loss = 0.03471809, step = 2043 (0.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.113\n",
      "INFO:tensorflow:loss = 0.034097277, step = 2143 (0.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.927\n",
      "INFO:tensorflow:loss = 0.033989977, step = 2243 (0.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.398\n",
      "INFO:tensorflow:loss = 0.034878407, step = 2343 (0.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.048\n",
      "INFO:tensorflow:loss = 0.03457252, step = 2443 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.727\n",
      "INFO:tensorflow:loss = 0.034128584, step = 2543 (0.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.494\n",
      "INFO:tensorflow:loss = 0.03409593, step = 2643 (0.420 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.222\n",
      "INFO:tensorflow:loss = 0.03476888, step = 2743 (0.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.835\n",
      "INFO:tensorflow:loss = 0.035382595, step = 2843 (0.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.858\n",
      "INFO:tensorflow:loss = 0.034152426, step = 2943 (0.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.664\n",
      "INFO:tensorflow:loss = 0.033890456, step = 3043 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.937\n",
      "INFO:tensorflow:loss = 0.033136956, step = 3143 (0.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 232.779\n",
      "INFO:tensorflow:loss = 0.033701632, step = 3243 (0.429 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.711\n",
      "INFO:tensorflow:loss = 0.034541808, step = 3343 (0.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.271\n",
      "INFO:tensorflow:loss = 0.034742415, step = 3443 (0.421 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.137\n",
      "INFO:tensorflow:loss = 0.03386067, step = 3543 (0.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.374\n",
      "INFO:tensorflow:loss = 0.034995135, step = 3643 (0.436 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3684 into /tmp/tf_bow_sst_20181202-0353/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.032155655.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-02-03:54:20\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181202-0353/model.ckpt-3684\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-02-03:54:22\n",
      "INFO:tensorflow:Saving dict for global step 3684: accuracy = 1.0, cross_entropy_loss = 0.0052925767, global_step = 3684, loss = 0.034171924\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3684: /tmp/tf_bow_sst_20181202-0353/model.ckpt-3684\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181202-0353/model.ckpt-3684\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3684 into /tmp/tf_bow_sst_20181202-0353/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.033747055, step = 3685\n",
      "INFO:tensorflow:global_step/sec: 204.999\n",
      "INFO:tensorflow:loss = 0.034115948, step = 3785 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 232.007\n",
      "INFO:tensorflow:loss = 0.03446347, step = 3885 (0.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.308\n",
      "INFO:tensorflow:loss = 0.033899315, step = 3985 (0.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.16\n",
      "INFO:tensorflow:loss = 0.03382215, step = 4085 (0.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.43\n",
      "INFO:tensorflow:loss = 0.03468569, step = 4185 (0.420 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.353\n",
      "INFO:tensorflow:loss = 0.034412317, step = 4285 (0.434 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.111\n",
      "INFO:tensorflow:loss = 0.03399677, step = 4385 (0.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.674\n",
      "INFO:tensorflow:loss = 0.033974487, step = 4485 (0.443 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.128\n",
      "INFO:tensorflow:loss = 0.034644052, step = 4585 (0.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.109\n",
      "INFO:tensorflow:loss = 0.03524711, step = 4685 (0.614 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.748\n",
      "INFO:tensorflow:loss = 0.03405595, step = 4785 (0.606 sec)\n",
      "INFO:tensorflow:global_step/sec: 167.466\n",
      "INFO:tensorflow:loss = 0.03379809, step = 4885 (0.597 sec)\n",
      "INFO:tensorflow:global_step/sec: 170.926\n",
      "INFO:tensorflow:loss = 0.033064947, step = 4985 (0.586 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.583\n",
      "INFO:tensorflow:loss = 0.033621695, step = 5085 (0.611 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.367\n",
      "INFO:tensorflow:loss = 0.03446731, step = 5185 (0.613 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.042\n",
      "INFO:tensorflow:loss = 0.034653682, step = 5285 (0.591 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.935\n",
      "INFO:tensorflow:loss = 0.033799678, step = 5385 (0.621 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 164.979\n",
      "INFO:tensorflow:loss = 0.034935027, step = 5485 (0.606 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5526 into /tmp/tf_bow_sst_20181202-0353/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0321045.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-02-03:54:42\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181202-0353/model.ckpt-5526\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-02-03:54:45\n",
      "INFO:tensorflow:Saving dict for global step 5526: accuracy = 1.0, cross_entropy_loss = 0.0052720606, global_step = 5526, loss = 0.034114897\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5526: /tmp/tf_bow_sst_20181202-0353/model.ckpt-5526\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181202-0353/model.ckpt-5526\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 5526 into /tmp/tf_bow_sst_20181202-0353/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.033691898, step = 5527\n",
      "INFO:tensorflow:global_step/sec: 210.277\n",
      "INFO:tensorflow:loss = 0.034063812, step = 5627 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.56\n",
      "INFO:tensorflow:loss = 0.034414817, step = 5727 (0.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 232.07\n",
      "INFO:tensorflow:loss = 0.03385408, step = 5827 (0.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.704\n",
      "INFO:tensorflow:loss = 0.033779584, step = 5927 (0.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.779\n",
      "INFO:tensorflow:loss = 0.03464771, step = 6027 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.721\n",
      "INFO:tensorflow:loss = 0.034376483, step = 6127 (0.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.887\n",
      "INFO:tensorflow:loss = 0.033962604, step = 6227 (0.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 232.16\n",
      "INFO:tensorflow:loss = 0.0339429, step = 6327 (0.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.252\n",
      "INFO:tensorflow:loss = 0.034616362, step = 6427 (0.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.328\n",
      "INFO:tensorflow:loss = 0.035221204, step = 6527 (0.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.115\n",
      "INFO:tensorflow:loss = 0.034030907, step = 6627 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.456\n",
      "INFO:tensorflow:loss = 0.033771582, step = 6727 (0.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.933\n",
      "INFO:tensorflow:loss = 0.033039737, step = 6827 (0.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.919\n",
      "INFO:tensorflow:loss = 0.033598345, step = 6927 (0.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.78\n",
      "INFO:tensorflow:loss = 0.034450427, step = 7027 (0.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.982\n",
      "INFO:tensorflow:loss = 0.03462732, step = 7127 (0.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.943\n",
      "INFO:tensorflow:loss = 0.033783186, step = 7227 (0.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.744\n",
      "INFO:tensorflow:loss = 0.034923874, step = 7327 (0.437 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7368 into /tmp/tf_bow_sst_20181202-0353/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.03208383.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-02-03:54:54\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181202-0353/model.ckpt-7368\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-02-03:54:57\n",
      "INFO:tensorflow:Saving dict for global step 7368: accuracy = 1.0, cross_entropy_loss = 0.0052656806, global_step = 7368, loss = 0.034100622\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 7368: /tmp/tf_bow_sst_20181202-0353/model.ckpt-7368\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181202-0353/model.ckpt-7368\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 7368 into /tmp/tf_bow_sst_20181202-0353/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.033676486, step = 7369\n",
      "INFO:tensorflow:global_step/sec: 207.055\n",
      "INFO:tensorflow:loss = 0.03405072, step = 7469 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.566\n",
      "INFO:tensorflow:loss = 0.03440413, step = 7569 (0.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.921\n",
      "INFO:tensorflow:loss = 0.03384246, step = 7669 (0.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.09\n",
      "INFO:tensorflow:loss = 0.033768173, step = 7769 (0.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.933\n",
      "INFO:tensorflow:loss = 0.034640837, step = 7869 (0.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.213\n",
      "INFO:tensorflow:loss = 0.034369033, step = 7969 (0.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.413\n",
      "INFO:tensorflow:loss = 0.033954393, step = 8069 (0.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 232.258\n",
      "INFO:tensorflow:loss = 0.03393546, step = 8169 (0.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.313\n",
      "INFO:tensorflow:loss = 0.034611426, step = 8269 (0.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.214\n",
      "INFO:tensorflow:loss = 0.035217945, step = 8369 (0.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.62\n",
      "INFO:tensorflow:loss = 0.034025423, step = 8469 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.81\n",
      "INFO:tensorflow:loss = 0.03376482, step = 8569 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.521\n",
      "INFO:tensorflow:loss = 0.033032205, step = 8669 (0.429 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.038\n",
      "INFO:tensorflow:loss = 0.03359226, step = 8769 (0.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.926\n",
      "INFO:tensorflow:loss = 0.034447644, step = 8869 (0.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.299\n",
      "INFO:tensorflow:loss = 0.03461769, step = 8969 (0.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.949\n",
      "INFO:tensorflow:loss = 0.033779386, step = 9069 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.264\n",
      "INFO:tensorflow:loss = 0.034923024, step = 9169 (0.419 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9210 into /tmp/tf_bow_sst_20181202-0353/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.032076724.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-02-03:55:06\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181202-0353/model.ckpt-9210\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-02-03:55:08\n",
      "INFO:tensorflow:Saving dict for global step 9210: accuracy = 1.0, cross_entropy_loss = 0.0052639386, global_step = 9210, loss = 0.0340976\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 9210: /tmp/tf_bow_sst_20181202-0353/model.ckpt-9210\n"
     ]
    }
   ],
   "source": [
    "# Training params, just used in this cell for the input_fn-s\n",
    "train_params = dict(batch_size=25, total_epochs=10, eval_every=2)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Construct and train the model, saving checkpoints to the directory above.\n",
    "# Input function for training set batches\n",
    "# Do 'eval_every' epochs at once, followed by evaluating on the dev set.\n",
    "# NOTE: use patch_numpy_io.numpy_input_fn instead of tf.estimator.inputs.numpy_input_fn\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": train_x, \"ns\": train_n}, y=train_y,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n",
    "\n",
    "# Input function for dev set batches. As above, but:\n",
    "# - Don't randomize order\n",
    "# - Iterate exactly once (one epoch)\n",
    "dev_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": dev_x, \"ns\": dev_n}, y=dev_y,\n",
    "                    batch_size=25, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on dev\n",
    "    model.train(input_fn=train_input_fn)\n",
    "    eval_metrics = model.evaluate(input_fn=dev_input_fn, name=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-02-03:57:10\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181202-0353/model.ckpt-9210\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-02-03:57:10\n",
      "INFO:tensorflow:Saving dict for global step 9210: accuracy = 1.0, cross_entropy_loss = 0.0052822744, global_step = 9210, loss = 0.05611941\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 9210: /tmp/tf_bow_sst_20181202-0353/model.ckpt-9210\n",
      "Accuracy on test set: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 1.0,\n",
       " 'cross_entropy_loss': 0.0052822744,\n",
       " 'loss': 0.05611941,\n",
       " 'global_step': 9210}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# test_input_fn = None  # replace with an input_fn, similar to dev_input_fn\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": test_x, \"ns\": test_n}, y=test_y,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn = test_input_fn)  # replace with result of model.evaluate(...)\n",
    "\n",
    "#### END(YOUR CODE) ####\n",
    "print(\"Accuracy on test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181202-0353/model.ckpt-9210\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Accuracy on test set: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predictions = list(model.predict(test_input_fn))  # list of dicts\n",
    "y_pred = [p['max'] for p in predictions]\n",
    "acc = accuracy_score(y_pred, test_y)\n",
    "\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[{'proba': array([0.0054353 , 0.99456465], dtype=float32), 'max': 1}, {'proba': array([0.00532511, 0.9946749 ], dtype=float32), 'max': 1}]\n"
     ]
    }
   ],
   "source": [
    "# predicted labels\n",
    "print(y_pred[:50])\n",
    "print(predictions[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
