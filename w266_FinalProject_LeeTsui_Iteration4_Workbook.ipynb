{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felixtsui/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "##This Iteration calculates the QoQ revenue change as the y_tag\n",
    "import dropbox\n",
    "import urllib.request, json \n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "import datetime as DT\n",
    "import ast\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from string import punctuation\n",
    "import csv\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary\n",
    "\n",
    "# Import pandas and tensorflow\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.10\"))\n",
    "\n",
    "# Import model\n",
    "import models\n",
    "\n",
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import misc\n",
    "from importlib import reload\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "import itertools\n",
    "from w266_common import patched_numpy_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Download the pre-processed tweets from dropbox\n",
    "\n",
    "access_token = 'p3Ze8FpdRTAAAAAAAAAADb9hCQ8aXXYU3A3gGM1HEXyeMrs8KI2SrA71KDmmCXte'\n",
    "dbx = dropbox.Dropbox(access_token)\n",
    "\n",
    "\n",
    "\n",
    "access_token2 = 'm_fKi8mWZ0AAAAAAAAAADlfzTm37f1y45k92Xpiw1B6mjT3dQqzNvKgpIUSR17uq'\n",
    "dbx2 = dropbox.Dropbox(access_token2)\n",
    "\n",
    "# #Write the combined file into the drive\n",
    "with open(\"InputX.txt\", \"wb\") as f:\n",
    "    metadata, res = dbx.files_download(path=\"/Inputdata2.txt\")\n",
    "    metadata2, res2 = dbx2.files_download(path=\"/Inputdata.txt\")\n",
    "    f.write(res.content)\n",
    "    f.write(res2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Translate date of the tweet to quarter reference\n",
    "def Quarterlookup(date):\n",
    "    if date>=DT.datetime(2018,10,1):\n",
    "        return '18Q4'\n",
    "    elif date>=DT.datetime(2018,7,1):\n",
    "        return '18Q3'\n",
    "    elif date>=DT.datetime(2018,4,1):\n",
    "        return '18Q2'\n",
    "    elif date>=DT.datetime(2018,1,1):\n",
    "        return '18Q1'\n",
    "    elif date>=DT.datetime(2017,10,1):\n",
    "        return '17Q4'\n",
    "    elif date>=DT.datetime(2017,7,1):\n",
    "        return '17Q3'\n",
    "    elif date>=DT.datetime(2017,4,1):\n",
    "        return '17Q2'\n",
    "    elif date>=DT.datetime(2017,1,1):\n",
    "        return '17Q1'\n",
    "    elif date>=DT.datetime(2016,10,1):\n",
    "        return '16Q4'\n",
    "    elif date>=DT.datetime(2016,7,1):\n",
    "        return '16Q3'\n",
    "    elif date>=DT.datetime(2016,4,1):\n",
    "        return '16Q2'\n",
    "    elif date>=DT.datetime(2016,1,1):\n",
    "        return '16Q1'\n",
    "    elif date>=DT.datetime(2015,10,1):\n",
    "        return '15Q4'\n",
    "    elif date>=DT.datetime(2015,7,1):\n",
    "        return '15Q3'\n",
    "    elif date>=DT.datetime(2015,4,1):\n",
    "        return '15Q2'\n",
    "    else:\n",
    "        return '15Q1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reading the Input X file and process it\n",
    "d=defaultdict(list)\n",
    "\n",
    "with open('InputX.txt') as f:\n",
    "    for line in f:\n",
    "        count=0\n",
    "        temp =-1\n",
    "        k,v=line.split(':',1)\n",
    "        while True:\n",
    "            \n",
    "            oldtemp=temp\n",
    "            temp=v.find(']]',temp+1)\n",
    "            if count==0:\n",
    "                d[k].append(v[oldtemp+2:temp+2]) \n",
    "            else:\n",
    "                d[k].append(v[oldtemp+4:temp+2])\n",
    "           \n",
    "             \n",
    "            count+=1\n",
    "            if temp == -1: break\n",
    "            #if count == 500: break  # initially limit for time sake\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read from the pre prepared vocab file\n",
    "allword=set()\n",
    "with open('vocab1.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        allword.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5,855\n"
     ]
    }
   ],
   "source": [
    "allword=sorted(allword)\n",
    "vocab = vocabulary.Vocabulary(allword, size=None)  \n",
    "print(\"Vocabulary size: {:,}\".format(vocab.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the padding\n",
    "def pad_np_array(example_ids, max_len=50, pad_id=0):\n",
    "\n",
    "    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\n",
    "    ns = np.zeros([len(example_ids)], dtype=np.int32)\n",
    "    cpy_len = min(len(example_ids), max_len)\n",
    "\n",
    "    arr = example_ids[:cpy_len]\n",
    "    ns = cpy_len\n",
    "    return arr, ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary to collect the tweet detail and the length of each tweet\n",
    "InputX=defaultdict(dict)\n",
    "InputN=defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set of all the stocks of interest\n",
    "#trainstock =['CVSHealth', 'RiteAid', \"Conn's\", \"Casey's\", 'Kroger','WeisMarkets', 'HavertyFurniture', \"Kirkland's\", 'Pier1Imports', 'BuildersFirstSource', 'HuttigBuildingProducts', 'LumberLiquidators', 'TransWorldEntertainment', 'MarineMax', 'Medifast', 'SallyBeauty',\"Sotheby's\", 'StarGasPartners', 'SuburbanPropanePartners', 'TitanMachinery', 'TractorSupply', 'TravelCentersofAmerica', 'Big5SportingGoods', \"Dick'sSportingGoods\", 'HibbettSports', 'Build-A-BearWorkshop']\n",
    "#teststock =['CVSHealth', 'RiteAid', \"Conn's\", \"Casey's\", 'Kroger','WeisMarkets', 'HavertyFurniture', \"Kirkland's\", 'Pier1Imports', 'BuildersFirstSource', 'HuttigBuildingProducts', 'LumberLiquidators', 'TransWorldEntertainment', 'MarineMax', 'Medifast', 'SallyBeauty',\"Sotheby's\", 'StarGasPartners', 'SuburbanPropanePartners', 'TitanMachinery', 'TractorSupply', 'TravelCentersofAmerica', 'Big5SportingGoods', \"Dick'sSportingGoods\", 'HibbettSports', 'Build-A-BearWorkshop']\n",
    "\n",
    "trainstock =[\"Medifast\"]\n",
    "teststock =trainstock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Converting the string of data into dictionary\n",
    "\n",
    "\n",
    "for key,value in d.items():\n",
    "    Inputx=defaultdict(list)\n",
    "    Inputn=defaultdict(list)\n",
    "    if key in trainstock:\n",
    "        for value2 in value:\n",
    "        \n",
    "            temp=value2.find('), ')\n",
    "            date=value2[14:temp+3]\n",
    "            try:\n",
    "                year=int(date[1:5])\n",
    "                month=int(date[date.find(',')+2:date.find(',',date.find(',')+1)])\n",
    "                day=int(date[date.find(')')-2:date.find(')')])\n",
    "            \n",
    "                sentence, length = pad_np_array (vocab.words_to_ids(ast.literal_eval(value2[temp+3:-1])))\n",
    "                ##Storing in content of each tweet and length of each tweet in sub level variable\n",
    "                Inputx[Quarterlookup(DT.datetime(year,month,day))].append(sentence)\n",
    "                Inputn[Quarterlookup(DT.datetime(year,month,day))].append(length)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    ##Storing in content of each tweet and length of each tweet in the top level variable\n",
    "    InputX[key]=OrderedDict(sorted(Inputx.items()))\n",
    "    InputN[key]=OrderedDict(sorted(Inputn.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CVSHealth': 'CVS', 'RiteAid': 'RAD', \"Conn's\": 'CONN', \"Casey's\": 'CASY', 'Kroger': 'KR', 'WeisMarkets': 'WMK', 'HavertyFurniture': 'HVT', \"Kirkland's\": 'KIRK', 'Pier1Imports': 'PIR', 'BuildersFirstSource': 'BLDR', 'HuttigBuildingProducts': 'HBP', 'LumberLiquidators': 'LL', 'TransWorldEntertainment': 'TWMC', 'MarineMax': 'HZO', 'Medifast': 'MED', 'SallyBeauty': 'SBH', \"Sotheby's\": 'BID', 'StarGasPartners': 'SGU', 'SuburbanPropanePartners': 'SPH', 'TitanMachinery': 'TITN', 'TractorSupply': 'TSCO', 'TravelCentersofAmerica': 'TA', 'Big5SportingGoods': 'BGFV', \"Dick'sSportingGoods\": 'DKS', 'HibbettSports': 'HIBB', 'Build-A-BearWorkshop': 'BBW'}\n"
     ]
    }
   ],
   "source": [
    "##List of stockname and relevant stock code\n",
    "l = [(\"CVSHealth\", \"CVS\"),\n",
    "(\"RiteAid\", \"RAD\"),\n",
    "(\"Conn's\", \"CONN\"),\n",
    "(\"Casey's\", \"CASY\"),\n",
    "(\"Kroger\", \"KR\"),\n",
    "(\"WeisMarkets\", \"WMK\"),\n",
    "(\"HavertyFurniture\", \"HVT\"),\n",
    "(\"Kirkland's\", \"KIRK\"),\n",
    "(\"Pier1Imports\",\"PIR\"),\n",
    "(\"BuildersFirstSource\",\"BLDR\"),\n",
    "(\"HuttigBuildingProducts\", \"HBP\"),\n",
    "(\"LumberLiquidators\", \"LL\"),\n",
    "(\"TransWorldEntertainment\", \"TWMC\"),\n",
    "(\"MarineMax\", \"HZO\"),\n",
    "(\"Medifast\", \"MED\"),\n",
    "(\"SallyBeauty\", \"SBH\"),\n",
    "(\"Sotheby's\", \"BID\"),\n",
    "(\"StarGasPartners\", \"SGU\"),\n",
    "(\"SuburbanPropanePartners\", \"SPH\"),\n",
    "(\"TitanMachinery\", \"TITN\"),\n",
    "(\"TractorSupply\",\"TSCO\"),\n",
    "(\"TravelCentersofAmerica\", \"TA\"),\n",
    "(\"Big5SportingGoods\", \"BGFV\"),\n",
    "(\"Dick'sSportingGoods\", \"DKS\"),\n",
    "(\"HibbettSports\", \"HIBB\"),\n",
    "(\"Build-A-BearWorkshop\", \"BBW\")]\n",
    "     \n",
    "all_stock = {}\n",
    "[all_stock.update({k:v}) for k,v in l]\n",
    "print(all_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Read the quarterly return detail from edgaronline and convert into library\n",
    "top=\"http://datafied.api.edgar-online.com/v2/corefinancials/qtr?primarysymbols=\"\n",
    "bottom=\"&appkey=asc97xrhkyu4959aptu76zxj\"\n",
    "Outputy =defaultdict(dict)\n",
    "for stockname,stock in all_stock.items():\n",
    "    check=top+stock+bottom\n",
    "    with urllib.request.urlopen(check) as url:\n",
    "        data = json.loads(url.read().decode())\n",
    "    count1=0\n",
    "    count2=0\n",
    "    end_date=[]\n",
    "    revenue_change=[]\n",
    "    last_rev=0\n",
    "\n",
    "    for key, value in data.items():\n",
    "        \n",
    "        for key2, value2 in value.items():          \n",
    "        \n",
    "            if count1!=0:  \n",
    "                for i in reversed(value2):\n",
    "                  \n",
    "                    for j in i[\"values\"]:\n",
    "                     \n",
    "                        if count2!=0:   \n",
    "                            if j[\"field\"] ==\"periodenddate\":\n",
    "                                end_date.append(Quarterlookup(DT.datetime.strptime(j['value'], '%m/%d/%Y')))\n",
    "                       \n",
    "                            ##Calculate change of revenue into the y_label \n",
    "                            if j[\"field\"] ==\"totalrevenue\":\n",
    "                                revenue_cal =round(float(j['value'])/float(last_rev) - 1,3)\n",
    "                                last_rev=j['value']\n",
    "                                if revenue_cal>0:\n",
    "                                    revenue_change.append(1)\n",
    "                                else:\n",
    "                                    revenue_change.append(0)\n",
    "                        else:\n",
    "                            ##Storing the initial quarter of revenue for calculation\n",
    "                            if j[\"field\"] ==\"totalrevenue\":      \n",
    "                                last_rev=j['value']\n",
    "                              \n",
    "                    count2+=1\n",
    "            count1+=1\n",
    "            \n",
    "    ##Output will be a dictionary with stockname\n",
    "    Outputy[stockname][\"period\"]=end_date\n",
    "    Outputy[stockname][\"revenuechange\"]=revenue_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Writing y_tag to file\n",
    "with open('Outputy.txt', 'w') as f:\n",
    "    print(Outputy, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating the train/Dev/test var\n",
    "train_x_init = []\n",
    "train_n_init = []\n",
    "train_y_init = []\n",
    "dev_x_init = []\n",
    "dev_n_init = []\n",
    "dev_y_init = []\n",
    "test_x_init = []\n",
    "test_n_init = []\n",
    "test_y_init = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Putting the earlier dictinoary into the train var in order of quarter\n",
    "trainquarter=['15Q1','15Q2','15Q3','15Q4','16Q1','16Q2','16Q3','16Q4','17Q1','17Q2','17Q3','17Q4','18Q1']\n",
    "\n",
    "for i in trainstock:\n",
    "    for j in range(len(trainquarter)):\n",
    "        try:\n",
    "                train_x_init.append(InputX [i][trainquarter[j]])\n",
    "                train_n_init.append(InputN [i][trainquarter[j]])\n",
    "                length=len(InputN [i][trainquarter[j]])           \n",
    "                train_y_init.append([Outputy [i]['revenuechange'][j]]*length)\n",
    "      \n",
    "        except:\n",
    "            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Putting the earlier dictinoary into the dev/test var in order of quarter\n",
    "\n",
    "testquarter=['18Q2','18Q3']\n",
    "\n",
    "for i in teststock:\n",
    "    for j in range(len(testquarter)):\n",
    "        try:\n",
    "            if testquarter[j]=='18Q3':\n",
    "                test_x_init.append(InputX [i][testquarter[j]])\n",
    "                test_n_init.append(InputN [i][testquarter[j]])\n",
    "                length=len(InputN [i][testquarter[j]])\n",
    "                test_y_init.append([Outputy [i]['revenuechange'][j]]*length)   \n",
    "            \n",
    "            elif testquarter[j]=='18Q2':\n",
    "                dev_x_init.append(InputX [i][testquarter[j]])\n",
    "                dev_n_init.append(InputN [i][testquarter[j]])\n",
    "                length=len(InputN [i][testquarter[j]])\n",
    "                dev_y_init.append([Outputy [i]['revenuechange'][j]]*length)     \n",
    "               \n",
    "      \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating the refined train/Dev/test var by flattening the shape of the original var\n",
    "train_x_init2 = []\n",
    "train_n_init2 = []\n",
    "train_y_init2 = []\n",
    "dev_x_init2 = []\n",
    "dev_n_init2 = []\n",
    "dev_y_init2 = []\n",
    "test_x_init2 = []\n",
    "test_n_init2 = []\n",
    "test_y_init2 = []\n",
    "\n",
    "for i in range(len(train_x_init)):\n",
    "    for j in range(len(train_x_init[i])):\n",
    "        train_x_init2.append(train_x_init[i][j])\n",
    "        train_n_init2.append(train_n_init[i][j])\n",
    "        train_y_init2.append(train_y_init[i][j])\n",
    "        \n",
    "for i in range(len(dev_x_init)):\n",
    "    for j in range(len(dev_x_init[i])):\n",
    "        dev_x_init2.append(dev_x_init[i][j])\n",
    "        dev_n_init2.append(dev_n_init[i][j])\n",
    "        dev_y_init2.append(dev_y_init[i][j])\n",
    "        \n",
    "for i in range(len(test_x_init)):\n",
    "    for j in range(len(test_x_init[i])):\n",
    "        test_x_init2.append(test_x_init[i][j])\n",
    "        test_n_init2.append(test_n_init[i][j])\n",
    "        test_y_init2.append(test_y_init[i][j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL BAG OF WORDS MODEL #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Padding the refined var\n",
    "def pad_np_array2(example_ids, max_len=50, pad_id=0):\n",
    "\n",
    "    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\n",
    "    ns = np.zeros([len(example_ids)], dtype=np.int32)\n",
    "    for i, ids in enumerate(example_ids):\n",
    "        cpy_len = min(len(ids), max_len)\n",
    "        arr[i,:cpy_len] = ids[:cpy_len]\n",
    "        ns[i] = cpy_len\n",
    "    return arr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating the final refined train/Dev/test var by padding it\n",
    "train_x = pad_np_array2(train_x_init2, max_len = 50, pad_id=0)\n",
    "train_n = np.array(train_n_init2)\n",
    "train_y = np.array(train_y_init2)\n",
    "\n",
    "test_x = pad_np_array2(test_x_init2, max_len = 50, pad_id=0)\n",
    "test_n = np.array(test_n_init2)\n",
    "test_y = np.array(test_y_init2)\n",
    "\n",
    "dev_x = pad_np_array2(dev_x_init2, max_len = 50, pad_id=0)\n",
    "dev_n = np.array(dev_n_init2)\n",
    "dev_y = np.array(dev_y_init2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 27376)]\n",
      "[0]\n",
      "[27376]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-208-035e641944fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlbl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcnts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlbl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "##Creat histogram to see the distribution of the testing data tag\n",
    "lbl, cnts = np.unique(test_y, return_counts=True)\n",
    "print(list(zip(lbl, cnts)))\n",
    "print(lbl)\n",
    "print(cnts)\n",
    "\n",
    "plt.hist((cnts[0]*[lbl[0]],cnts[1]*[lbl[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Creat histogram to see the distribution of the training data tag\n",
    "lbl2, cnts2 = np.unique(train_y, return_counts=True)\n",
    "print(list(zip(lbl2, cnts2)))\n",
    "print(lbl2[0])\n",
    "print(cnts2[0])\n",
    "\n",
    "# labels = \n",
    "# counts = \n",
    "\n",
    "plt.hist((cnts2[0]*[lbl2[0]],cnts2[1]*[lbl2[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15,625 examples, moving-average loss 0.39\n",
      "31,250 examples, moving-average loss 0.33\n",
      "46,875 examples, moving-average loss 0.30\n",
      "62,500 examples, moving-average loss 0.29\n",
      "78,125 examples, moving-average loss 0.29\n",
      "93,750 examples, moving-average loss 0.33\n",
      "109,375 examples, moving-average loss 0.35\n",
      "125,000 examples, moving-average loss 0.30\n",
      "140,625 examples, moving-average loss 0.29\n",
      "156,250 examples, moving-average loss 0.29\n",
      "171,875 examples, moving-average loss 0.38\n",
      "187,500 examples, moving-average loss 0.31\n",
      "203,125 examples, moving-average loss 0.30\n",
      "218,750 examples, moving-average loss 0.30\n",
      "234,375 examples, moving-average loss 0.30\n",
      "250,000 examples, moving-average loss 0.29\n",
      "Completed one epoch in 0:00:09\n"
     ]
    }
   ],
   "source": [
    "##Setting up the NBoW Model\n",
    "reload(models)\n",
    "\n",
    "x, ns, y = train_x, train_n, train_y\n",
    "batch_size = 125\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=vocab.size, embed_dim=25, hidden_dims=[20,20], num_classes=3,\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.1)\n",
    "model_fn = models.classifier_model_fn\n",
    "\n",
    "total_batches = 0\n",
    "total_examples = 0\n",
    "total_loss = 0\n",
    "loss_ema = np.log(2)  # track exponential-moving-average of loss\n",
    "ema_decay = np.exp(-1/10)  # decay parameter for moving average = np.exp(-1/history_length)\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    ##\n",
    "    # Construct the graph here. No session.run calls - just wiring up Tensors.\n",
    "    ##\n",
    "    # Add placeholders so we can feed in data.\n",
    "    x_ph_  = tf.placeholder(tf.int32, shape=[None, x.shape[1]])  # [batch_size, max_len]\n",
    "    ns_ph_ = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    y_ph_  = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    \n",
    "    # Construct the graph using model_fn\n",
    "    features = {\"ids\": x_ph_, \"ns\": ns_ph_}  # note that values are Tensors\n",
    "    estimator_spec = model_fn(features, labels=y_ph_, mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                              params=model_params)\n",
    "    loss_     = estimator_spec.loss\n",
    "    train_op_ = estimator_spec.train_op\n",
    "    \n",
    "    ##\n",
    "    # Done constructing the graph, now we can make session.run calls.\n",
    "    ##\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Run a single epoch\n",
    "    t0 = time.time()\n",
    "    for (bx, bns, by) in utils.multi_batch_generator(batch_size, x, ns, y):\n",
    "        # feed NumPy arrays into the placeholder Tensors\n",
    "        feed_dict = {x_ph_: bx, ns_ph_: bns, y_ph_: by}\n",
    "        batch_loss, _ = sess.run([loss_, train_op_], feed_dict=feed_dict)\n",
    "        \n",
    "        # Compute some statistics\n",
    "        total_batches += 1\n",
    "        total_examples += len(bx)\n",
    "        total_loss += batch_loss * len(bx)  # re-scale, since batch loss is mean\n",
    "        # Compute moving average to smooth out noisy per-batch loss\n",
    "        loss_ema = ema_decay * loss_ema + (1 - ema_decay) * batch_loss\n",
    "        \n",
    "        if (total_batches % 125 == 0):\n",
    "            print(\"{:5,} examples, moving-average loss {:.2f}\".format(total_examples, \n",
    "                                                                      loss_ema))    \n",
    "    print(\"Completed one epoch in {:s}\".format(utils.pretty_timedelta(since=t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (5,855 words) written to '/tmp/tf_bow_sst_20181208-2316/metadata.tsv'\n",
      "Projector config written to /tmp/tf_bow_sst_20181208-2316/projector_config.pbtxt\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tf_bow_sst_20181208-2316', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f281a005630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    tensorboard --logdir='/tmp/tf_bow_sst_20181208-2316' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "##Setting up the NBoW Model\n",
    "import models; reload(models)\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=vocab.size, embed_dim=25, hidden_dims=[20,20], num_classes=3,\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.1)\n",
    "\n",
    "checkpoint_dir = \"/tmp/tf_bow_sst_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "# Write vocabulary to file, so TensorBoard can label embeddings.\n",
    "# creates checkpoint_dir/projector_config.pbtxt and checkpoint_dir/metadata.tsv\n",
    "vocab.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=models.classifier_model_fn, \n",
    "                               params=model_params,\n",
    "                               model_dir=checkpoint_dir)\n",
    "print(\"\")\n",
    "print(\"To view training (once it starts), run:\\n\")\n",
    "print(\"    tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "print(\"\\nThen in your browser, open: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tf_bow_sst_20181208-2316/model.ckpt.\n",
      "INFO:tensorflow:loss = 5.464024, step = 1\n",
      "INFO:tensorflow:global_step/sec: 156.645\n",
      "INFO:tensorflow:loss = 0.39956146, step = 101 (0.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.592\n",
      "INFO:tensorflow:loss = 0.45825154, step = 201 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.578\n",
      "INFO:tensorflow:loss = 0.29790717, step = 301 (0.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.873\n",
      "INFO:tensorflow:loss = 0.31238127, step = 401 (0.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.109\n",
      "INFO:tensorflow:loss = 0.2979138, step = 501 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.045\n",
      "INFO:tensorflow:loss = 0.2935506, step = 601 (0.532 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.062\n",
      "INFO:tensorflow:loss = 0.291637, step = 701 (0.556 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.195\n",
      "INFO:tensorflow:loss = 0.29066312, step = 801 (0.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.975\n",
      "INFO:tensorflow:loss = 0.32543325, step = 901 (0.553 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.378\n",
      "INFO:tensorflow:loss = 0.3064547, step = 1001 (0.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.137\n",
      "INFO:tensorflow:loss = 0.2987535, step = 1101 (0.536 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.208\n",
      "INFO:tensorflow:loss = 0.2947523, step = 1201 (0.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.552\n",
      "INFO:tensorflow:loss = 0.29249558, step = 1301 (0.533 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.724\n",
      "INFO:tensorflow:loss = 0.29117087, step = 1401 (0.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.883\n",
      "INFO:tensorflow:loss = 0.29037204, step = 1501 (0.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.029\n",
      "INFO:tensorflow:loss = 0.28987905, step = 1601 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.438\n",
      "INFO:tensorflow:loss = 0.28956765, step = 1701 (0.536 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.326\n",
      "INFO:tensorflow:loss = 1.4014921, step = 1801 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.097\n",
      "INFO:tensorflow:loss = 0.32126495, step = 1901 (0.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.602\n",
      "INFO:tensorflow:loss = 0.3078248, step = 2001 (0.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.663\n",
      "INFO:tensorflow:loss = 0.30095255, step = 2101 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.088\n",
      "INFO:tensorflow:loss = 0.29679137, step = 2201 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.098\n",
      "INFO:tensorflow:loss = 1.0410078, step = 2301 (0.555 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.948\n",
      "INFO:tensorflow:loss = 0.3052657, step = 2401 (0.549 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.728\n",
      "INFO:tensorflow:loss = 0.38152453, step = 2501 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.444\n",
      "INFO:tensorflow:loss = 0.30198935, step = 2601 (0.536 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.304\n",
      "INFO:tensorflow:loss = 0.2970574, step = 2701 (0.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.456\n",
      "INFO:tensorflow:loss = 0.294666, step = 2801 (0.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.792\n",
      "INFO:tensorflow:loss = 0.29312366, step = 2901 (0.527 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.851\n",
      "INFO:tensorflow:loss = 0.35319945, step = 3001 (0.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.671\n",
      "INFO:tensorflow:loss = 0.31075567, step = 3101 (0.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.054\n",
      "INFO:tensorflow:loss = 0.30421007, step = 3201 (0.532 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.302\n",
      "INFO:tensorflow:loss = 0.3004283, step = 3301 (0.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.882\n",
      "INFO:tensorflow:loss = 0.29778314, step = 3401 (0.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.968\n",
      "INFO:tensorflow:loss = 0.2958161, step = 3501 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.943\n",
      "INFO:tensorflow:loss = 0.29432115, step = 3601 (0.549 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.472\n",
      "INFO:tensorflow:loss = 0.29317352, step = 3701 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.675\n",
      "INFO:tensorflow:loss = 0.29228783, step = 3801 (0.562 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.937\n",
      "INFO:tensorflow:loss = 1.5011313, step = 3901 (0.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.986\n",
      "INFO:tensorflow:loss = 0.33200476, step = 4001 (0.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.926\n",
      "INFO:tensorflow:loss = 0.31237262, step = 4101 (0.538 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.22\n",
      "INFO:tensorflow:loss = 0.30624205, step = 4201 (0.561 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4207 into /tmp/tf_bow_sst_20181208-2316/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.30596605.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-08-23:17:03\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181208-2316/model.ckpt-4207\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-08-23:17:04\n",
      "INFO:tensorflow:Saving dict for global step 4207: accuracy = 0.0, cross_entropy_loss = 3.2216856, global_step = 4207, loss = 3.4524517\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4207: /tmp/tf_bow_sst_20181208-2316/model.ckpt-4207\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181208-2316/model.ckpt-4207\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 4207 into /tmp/tf_bow_sst_20181208-2316/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.30592078, step = 4208\n",
      "INFO:tensorflow:global_step/sec: 165.832\n",
      "INFO:tensorflow:loss = 0.30217016, step = 4308 (0.605 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.371\n",
      "INFO:tensorflow:loss = 1.1073433, step = 4408 (0.554 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.654\n",
      "INFO:tensorflow:loss = 0.34723496, step = 4508 (0.566 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.715\n",
      "INFO:tensorflow:loss = 0.3626677, step = 4608 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.774\n",
      "INFO:tensorflow:loss = 0.32572812, step = 4708 (0.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.105\n",
      "INFO:tensorflow:loss = 0.3128335, step = 4808 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.644\n",
      "INFO:tensorflow:loss = 0.3063998, step = 4908 (0.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.285\n",
      "INFO:tensorflow:loss = 0.30252212, step = 5008 (0.549 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.774\n",
      "INFO:tensorflow:loss = 0.3745113, step = 5108 (0.532 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.998\n",
      "INFO:tensorflow:loss = 0.33088064, step = 5208 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.635\n",
      "INFO:tensorflow:loss = 0.31623197, step = 5308 (0.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.793\n",
      "INFO:tensorflow:loss = 0.30902267, step = 5408 (0.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.169\n",
      "INFO:tensorflow:loss = 0.30473563, step = 5508 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.538\n",
      "INFO:tensorflow:loss = 0.30179027, step = 5608 (0.543 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.38\n",
      "INFO:tensorflow:loss = 0.29957306, step = 5708 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.191\n",
      "INFO:tensorflow:loss = 0.29781806, step = 5808 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.356\n",
      "INFO:tensorflow:loss = 0.29639268, step = 5908 (0.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.751\n",
      "INFO:tensorflow:loss = 1.5133852, step = 6008 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.021\n",
      "INFO:tensorflow:loss = 0.36286628, step = 6108 (0.543 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.501\n",
      "INFO:tensorflow:loss = 0.33154345, step = 6208 (0.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.92\n",
      "INFO:tensorflow:loss = 0.31864583, step = 6308 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.723\n",
      "INFO:tensorflow:loss = 0.31149754, step = 6408 (0.562 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.635\n",
      "INFO:tensorflow:loss = 1.0451941, step = 6508 (0.533 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.805\n",
      "INFO:tensorflow:loss = 0.3484583, step = 6608 (0.538 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.3625671, step = 6708 (0.532 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.825\n",
      "INFO:tensorflow:loss = 0.32801592, step = 6808 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.025\n",
      "INFO:tensorflow:loss = 0.31519547, step = 6908 (0.532 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.773\n",
      "INFO:tensorflow:loss = 0.30834234, step = 7008 (0.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.477\n",
      "INFO:tensorflow:loss = 0.30414268, step = 7108 (0.557 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.282\n",
      "INFO:tensorflow:loss = 0.38428187, step = 7208 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.63\n",
      "INFO:tensorflow:loss = 0.33825517, step = 7308 (0.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.225\n",
      "INFO:tensorflow:loss = 0.32154983, step = 7408 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.928\n",
      "INFO:tensorflow:loss = 0.31294176, step = 7508 (0.562 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.528\n",
      "INFO:tensorflow:loss = 0.3077972, step = 7608 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.046\n",
      "INFO:tensorflow:loss = 0.30431545, step = 7708 (0.526 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.551\n",
      "INFO:tensorflow:loss = 0.30173284, step = 7808 (0.564 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.273\n",
      "INFO:tensorflow:loss = 0.2997009, step = 7908 (0.543 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.871\n",
      "INFO:tensorflow:loss = 0.29804572, step = 8008 (0.538 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.277\n",
      "INFO:tensorflow:loss = 1.5359889, step = 8108 (0.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.282\n",
      "INFO:tensorflow:loss = 0.37398645, step = 8208 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.653\n",
      "INFO:tensorflow:loss = 0.33900017, step = 8308 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.581\n",
      "INFO:tensorflow:loss = 0.3238192, step = 8408 (0.539 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8414 into /tmp/tf_bow_sst_20181208-2316/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.32317355.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-08-23:17:28\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181208-2316/model.ckpt-8414\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-08-23:17:29\n",
      "INFO:tensorflow:Saving dict for global step 8414: accuracy = 0.0, cross_entropy_loss = 3.2496395, global_step = 8414, loss = 3.5007377\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 8414: /tmp/tf_bow_sst_20181208-2316/model.ckpt-8414\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181208-2316/model.ckpt-8414\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 8414 into /tmp/tf_bow_sst_20181208-2316/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.32306808, step = 8415\n",
      "INFO:tensorflow:global_step/sec: 164.002\n",
      "INFO:tensorflow:loss = 0.31485507, step = 8515 (0.611 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.133\n",
      "INFO:tensorflow:loss = 0.9517984, step = 8615 (0.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.54\n",
      "INFO:tensorflow:loss = 0.3531868, step = 8715 (0.537 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.835\n",
      "INFO:tensorflow:loss = 0.36183685, step = 8815 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.409\n",
      "INFO:tensorflow:loss = 0.33007288, step = 8915 (0.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.918\n",
      "INFO:tensorflow:loss = 0.3175864, step = 9015 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.15\n",
      "INFO:tensorflow:loss = 0.31053162, step = 9115 (0.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.393\n",
      "INFO:tensorflow:loss = 0.30608228, step = 9215 (0.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.422\n",
      "INFO:tensorflow:loss = 0.3896395, step = 9315 (0.536 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.971\n",
      "INFO:tensorflow:loss = 0.34361482, step = 9415 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.309\n",
      "INFO:tensorflow:loss = 0.32558414, step = 9515 (0.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.128\n",
      "INFO:tensorflow:loss = 0.3158098, step = 9615 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.487\n",
      "INFO:tensorflow:loss = 0.30990225, step = 9715 (0.525 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.558\n",
      "INFO:tensorflow:loss = 0.30596948, step = 9815 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.666\n",
      "INFO:tensorflow:loss = 0.30312926, step = 9915 (0.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.525\n",
      "INFO:tensorflow:loss = 0.30094695, step = 10015 (0.560 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.974\n",
      "INFO:tensorflow:loss = 0.29919606, step = 10115 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.151\n",
      "INFO:tensorflow:loss = 1.5412258, step = 10215 (0.553 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.216\n",
      "INFO:tensorflow:loss = 0.37683496, step = 10315 (0.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.117\n",
      "INFO:tensorflow:loss = 0.34230098, step = 10415 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.052\n",
      "INFO:tensorflow:loss = 0.32617533, step = 10515 (0.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.981\n",
      "INFO:tensorflow:loss = 0.31703323, step = 10615 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.78\n",
      "INFO:tensorflow:loss = 0.98178154, step = 10715 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.508\n",
      "INFO:tensorflow:loss = 0.35634732, step = 10815 (0.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.799\n",
      "INFO:tensorflow:loss = 0.36788982, step = 10915 (0.556 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.344\n",
      "INFO:tensorflow:loss = 0.33247328, step = 11015 (0.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.589\n",
      "INFO:tensorflow:loss = 0.31991878, step = 11115 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.936\n",
      "INFO:tensorflow:loss = 0.31249592, step = 11215 (0.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.96\n",
      "INFO:tensorflow:loss = 0.30771282, step = 11315 (0.549 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.102\n",
      "INFO:tensorflow:loss = 0.39941764, step = 11415 (0.537 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.266\n",
      "INFO:tensorflow:loss = 0.34834927, step = 11515 (0.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.046\n",
      "INFO:tensorflow:loss = 0.3291626, step = 11615 (0.543 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.932\n",
      "INFO:tensorflow:loss = 0.31837094, step = 11715 (0.543 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.867\n",
      "INFO:tensorflow:loss = 0.31176996, step = 11815 (0.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.725\n",
      "INFO:tensorflow:loss = 0.30741408, step = 11915 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.307\n",
      "INFO:tensorflow:loss = 0.30432802, step = 12015 (0.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.937\n",
      "INFO:tensorflow:loss = 0.30200464, step = 12115 (0.559 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.295\n",
      "INFO:tensorflow:loss = 0.3001698, step = 12215 (0.543 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.669\n",
      "INFO:tensorflow:loss = 1.592551, step = 12315 (0.557 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.421\n",
      "INFO:tensorflow:loss = 0.3851927, step = 12415 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.942\n",
      "INFO:tensorflow:loss = 0.3480007, step = 12515 (0.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.857\n",
      "INFO:tensorflow:loss = 0.33050048, step = 12615 (0.540 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 12621 into /tmp/tf_bow_sst_20181208-2316/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.32972932.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-08-23:17:54\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181208-2316/model.ckpt-12621\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-08-23:17:55\n",
      "INFO:tensorflow:Saving dict for global step 12621: accuracy = 0.0, cross_entropy_loss = 3.2837288, global_step = 12621, loss = 3.54419\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 12621: /tmp/tf_bow_sst_20181208-2316/model.ckpt-12621\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181208-2316/model.ckpt-12621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 12621 into /tmp/tf_bow_sst_20181208-2316/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.32960317, step = 12622\n",
      "INFO:tensorflow:global_step/sec: 163.457\n",
      "INFO:tensorflow:loss = 0.31969684, step = 12722 (0.613 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.18\n",
      "INFO:tensorflow:loss = 0.9237328, step = 12822 (0.553 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.441\n",
      "INFO:tensorflow:loss = 0.35856715, step = 12922 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.137\n",
      "INFO:tensorflow:loss = 0.36843064, step = 13022 (0.537 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.434\n",
      "INFO:tensorflow:loss = 0.33420107, step = 13122 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.308\n",
      "INFO:tensorflow:loss = 0.32194662, step = 13222 (0.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.881\n",
      "INFO:tensorflow:loss = 0.31440738, step = 13322 (0.553 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.35\n",
      "INFO:tensorflow:loss = 0.30939484, step = 13422 (0.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.729\n",
      "INFO:tensorflow:loss = 0.4024782, step = 13522 (0.559 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.039\n",
      "INFO:tensorflow:loss = 0.3515057, step = 13622 (0.558 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.383\n",
      "INFO:tensorflow:loss = 0.3322511, step = 13722 (0.557 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.779\n",
      "INFO:tensorflow:loss = 0.32094005, step = 13822 (0.566 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.794\n",
      "INFO:tensorflow:loss = 0.3138051, step = 13922 (0.563 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.723\n",
      "INFO:tensorflow:loss = 0.30903104, step = 14022 (0.566 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.437\n",
      "INFO:tensorflow:loss = 0.3056489, step = 14122 (0.557 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.672\n",
      "INFO:tensorflow:loss = 0.30312365, step = 14222 (0.600 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.224\n",
      "INFO:tensorflow:loss = 0.30115214, step = 14322 (0.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.834\n",
      "INFO:tensorflow:loss = 1.585549, step = 14422 (0.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.844\n",
      "INFO:tensorflow:loss = 0.38878575, step = 14522 (0.555 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.998\n",
      "INFO:tensorflow:loss = 0.3516108, step = 14622 (0.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.547\n",
      "INFO:tensorflow:loss = 0.333794, step = 14722 (0.558 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.172\n",
      "INFO:tensorflow:loss = 0.32294744, step = 14822 (0.554 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.428\n",
      "INFO:tensorflow:loss = 0.96288383, step = 14922 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.276\n",
      "INFO:tensorflow:loss = 0.3621186, step = 15022 (0.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.075\n",
      "INFO:tensorflow:loss = 0.37694782, step = 15122 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.419\n",
      "INFO:tensorflow:loss = 0.33651698, step = 15222 (0.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.43\n",
      "INFO:tensorflow:loss = 0.32415268, step = 15322 (0.555 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.715\n",
      "INFO:tensorflow:loss = 0.316401, step = 15422 (0.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.886\n",
      "INFO:tensorflow:loss = 0.31112078, step = 15522 (0.555 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.33\n",
      "INFO:tensorflow:loss = 0.41413125, step = 15622 (0.543 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.012\n",
      "INFO:tensorflow:loss = 0.3555193, step = 15722 (0.565 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.626\n",
      "INFO:tensorflow:loss = 0.33580852, step = 15822 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.136\n",
      "INFO:tensorflow:loss = 0.32390398, step = 15922 (0.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.817\n",
      "INFO:tensorflow:loss = 0.31618685, step = 16022 (0.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.602\n",
      "INFO:tensorflow:loss = 0.31094164, step = 16122 (0.554 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.44\n",
      "INFO:tensorflow:loss = 0.3072077, step = 16222 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.507\n",
      "INFO:tensorflow:loss = 0.3044296, step = 16322 (0.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.474\n",
      "INFO:tensorflow:loss = 0.30227765, step = 16422 (0.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.348\n",
      "INFO:tensorflow:loss = 1.6405267, step = 16522 (0.537 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.463\n",
      "INFO:tensorflow:loss = 0.3958969, step = 16622 (0.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.616\n",
      "INFO:tensorflow:loss = 0.35574394, step = 16722 (0.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.9\n",
      "INFO:tensorflow:loss = 0.33736232, step = 16822 (0.569 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 16828 into /tmp/tf_bow_sst_20181208-2316/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.33652067.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-08-23:18:19\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181208-2316/model.ckpt-16828\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-08-23:18:20\n",
      "INFO:tensorflow:Saving dict for global step 16828: accuracy = 0.0, cross_entropy_loss = 3.2851634, global_step = 16828, loss = 3.5524697\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 16828: /tmp/tf_bow_sst_20181208-2316/model.ckpt-16828\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181208-2316/model.ckpt-16828\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 16828 into /tmp/tf_bow_sst_20181208-2316/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.33638272, step = 16829\n",
      "INFO:tensorflow:global_step/sec: 170.009\n",
      "INFO:tensorflow:loss = 0.3252374, step = 16929 (0.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.542\n",
      "INFO:tensorflow:loss = 0.91929805, step = 17029 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.339\n",
      "INFO:tensorflow:loss = 0.36294603, step = 17129 (0.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.342\n",
      "INFO:tensorflow:loss = 0.37687287, step = 17229 (0.558 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.023\n",
      "INFO:tensorflow:loss = 0.33766162, step = 17329 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.542\n",
      "INFO:tensorflow:loss = 0.32557863, step = 17429 (0.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.493\n",
      "INFO:tensorflow:loss = 0.3178696, step = 17529 (0.570 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.705\n",
      "INFO:tensorflow:loss = 0.3124951, step = 17629 (0.538 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.247\n",
      "INFO:tensorflow:loss = 0.41579625, step = 17729 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.59\n",
      "INFO:tensorflow:loss = 0.3573742, step = 17829 (0.564 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.401\n",
      "INFO:tensorflow:loss = 0.33800527, step = 17929 (0.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.38\n",
      "INFO:tensorflow:loss = 0.3260224, step = 18029 (0.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.621\n",
      "INFO:tensorflow:loss = 0.31804243, step = 18129 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.819\n",
      "INFO:tensorflow:loss = 0.31251186, step = 18229 (0.536 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.396\n",
      "INFO:tensorflow:loss = 0.3085307, step = 18329 (0.533 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.48\n",
      "INFO:tensorflow:loss = 0.3055564, step = 18429 (0.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.11\n",
      "INFO:tensorflow:loss = 0.30325538, step = 18529 (0.531 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.512\n",
      "INFO:tensorflow:loss = 1.6304706, step = 18629 (0.549 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.465\n",
      "INFO:tensorflow:loss = 0.39805207, step = 18729 (0.533 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.897\n",
      "INFO:tensorflow:loss = 0.35794234, step = 18829 (0.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.287\n",
      "INFO:tensorflow:loss = 0.33973467, step = 18929 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.453\n",
      "INFO:tensorflow:loss = 0.3280723, step = 19029 (0.558 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.298\n",
      "INFO:tensorflow:loss = 0.9660156, step = 19129 (0.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.95\n",
      "INFO:tensorflow:loss = 0.36601692, step = 19229 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.779\n",
      "INFO:tensorflow:loss = 0.38742656, step = 19329 (0.542 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 181.731\n",
      "INFO:tensorflow:loss = 0.3396719, step = 19429 (0.549 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.293\n",
      "INFO:tensorflow:loss = 0.32738113, step = 19529 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.843\n",
      "INFO:tensorflow:loss = 0.31955698, step = 19629 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.509\n",
      "INFO:tensorflow:loss = 0.31401488, step = 19729 (0.553 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.155\n",
      "INFO:tensorflow:loss = 0.43049213, step = 19829 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.512\n",
      "INFO:tensorflow:loss = 0.36074427, step = 19929 (0.560 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.199\n",
      "INFO:tensorflow:loss = 0.3409923, step = 20029 (0.533 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.419\n",
      "INFO:tensorflow:loss = 0.32869294, step = 20129 (0.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.501\n",
      "INFO:tensorflow:loss = 0.3203298, step = 20229 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.527\n",
      "INFO:tensorflow:loss = 0.3144347, step = 20329 (0.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.068\n",
      "INFO:tensorflow:loss = 0.3101453, step = 20429 (0.537 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.283\n",
      "INFO:tensorflow:loss = 0.3069245, step = 20529 (0.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.636\n",
      "INFO:tensorflow:loss = 0.30443197, step = 20629 (0.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.136\n",
      "INFO:tensorflow:loss = 1.6849129, step = 20729 (0.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.644\n",
      "INFO:tensorflow:loss = 0.40523857, step = 20829 (0.553 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.303\n",
      "INFO:tensorflow:loss = 0.36119613, step = 20929 (0.555 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.067\n",
      "INFO:tensorflow:loss = 0.34267235, step = 21029 (0.562 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 21035 into /tmp/tf_bow_sst_20181208-2316/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.34181032.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-08-23:18:45\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181208-2316/model.ckpt-21035\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-08-23:18:46\n",
      "INFO:tensorflow:Saving dict for global step 21035: accuracy = 0.0, cross_entropy_loss = 3.2856772, global_step = 21035, loss = 3.5579767\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 21035: /tmp/tf_bow_sst_20181208-2316/model.ckpt-21035\n"
     ]
    }
   ],
   "source": [
    "##Training the NBoW Model\n",
    "# Training params, just used in this cell for the input_fn-s\n",
    "train_params = dict(batch_size=125, total_epochs=10, eval_every=2)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Construct and train the model, saving checkpoints to the directory above.\n",
    "# Input function for training set batches\n",
    "# Do 'eval_every' epochs at once, followed by evaluating on the dev set.\n",
    "# NOTE: use patch_numpy_io.numpy_input_fn instead of tf.estimator.inputs.numpy_input_fn\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": train_x, \"ns\": train_n}, y=train_y,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n",
    "\n",
    "# Input function for dev set batches. As above, but:\n",
    "# - Don't randomize order\n",
    "# - Iterate exactly once (one epoch)\n",
    "dev_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": dev_x, \"ns\": dev_n}, y=dev_y,\n",
    "                    batch_size=125, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on dev\n",
    "    model.train(input_fn=train_input_fn)\n",
    "    eval_metrics = model.evaluate(input_fn=dev_input_fn, name=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-08-23:18:46\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181208-2316/model.ckpt-21035\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-08-23:18:47\n",
      "INFO:tensorflow:Saving dict for global step 21035: accuracy = 1.0, cross_entropy_loss = 0.06937858, global_step = 21035, loss = 0.34166822\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 21035: /tmp/tf_bow_sst_20181208-2316/model.ckpt-21035\n",
      "Accuracy on test set: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 1.0,\n",
       " 'cross_entropy_loss': 0.06937858,\n",
       " 'loss': 0.34166822,\n",
       " 'global_step': 21035}"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Calculating the Accuracy\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": test_x, \"ns\": test_n}, y=test_y,\n",
    "                    batch_size=125, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn = test_input_fn)  \n",
    "\n",
    "print(\"Accuracy on test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181208-2316/model.ckpt-21035\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Average precision score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felixtsui/anaconda3/lib/python3.6/site-packages/sklearn/metrics/ranking.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    }
   ],
   "source": [
    "##Calculating the Precision\n",
    "from sklearn.metrics import average_precision_score\n",
    "    \n",
    "predictions = list(model.predict(test_input_fn))  # list of dicts\n",
    "y_pred = [p['max'] for p in predictions]\n",
    "average_precision = average_precision_score(y_pred,test_y)\n",
    "\n",
    "print('Average precision score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medifast\n",
      "[(0, 20200)]\n",
      "0 vs 0.0\n"
     ]
    }
   ],
   "source": [
    "##Loss Analysis\n",
    "counttest=[]\n",
    "\n",
    "for i in range(len(test_y_init)):\n",
    "    counttest.append(len(test_y_init[i]))\n",
    "    \n",
    "count=0\n",
    "for i in range(len(counttest)):\n",
    " \n",
    "    lbl, cnts = np.unique(test_y[count:count+counttest[i]], return_counts=True)\n",
    "    lbl2, cnts2 = np.unique (y_pred[count:count+counttest[i]], return_counts=True)\n",
    "    print(teststock[i])\n",
    "    print(list(zip(lbl2, cnts2)))\n",
    "    print(lbl[0], \"vs\", round(sum(lbl2*cnts2)/sum(cnts2),0))\n",
    "    count+=counttest[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 25165)]\n",
      "[0]\n",
      "[25165]\n"
     ]
    }
   ],
   "source": [
    "##Histogram of the prediction\n",
    "lbl, cnts = np.unique(y_pred, return_counts=True)\n",
    "print(list(zip(lbl, cnts)))\n",
    "print(lbl)\n",
    "print(cnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
