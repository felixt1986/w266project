{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dropbox\n",
    "import urllib.request, json \n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "import datetime as DT\n",
    "import ast\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from string import punctuation\n",
    "import csv\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary\n",
    "\n",
    "########################### DLEE IMPORTS ##################################\n",
    "\n",
    "# Import pandas and tensorflow\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.10\"))\n",
    "\n",
    "# Import model\n",
    "import models\n",
    "\n",
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import misc\n",
    "from importlib import reload\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "import itertools\n",
    "from w266_common import patched_numpy_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Download the pre-processed tweets from dropbox\n",
    "\n",
    "# access_token = 'p3Ze8FpdRTAAAAAAAAAADb9hCQ8aXXYU3A3gGM1HEXyeMrs8KI2SrA71KDmmCXte'\n",
    "# dbx = dropbox.Dropbox(access_token)\n",
    "# metadata, res = dbx.files_download(path=\"/Inputdata2.txt\")\n",
    "\n",
    "\n",
    "# access_token2 = 'm_fKi8mWZ0AAAAAAAAAADlfzTm37f1y45k92Xpiw1B6mjT3dQqzNvKgpIUSR17uq'\n",
    "# dbx2 = dropbox.Dropbox(access_token2)\n",
    "\n",
    "# #Write the combined file into the drive\n",
    "# with open(\"InputX.txt\", \"wb\") as f:\n",
    "#     metadata, res = dbx.files_download(path=\"/Inputdata2.txt\")\n",
    "#     metadata2, res2 = dbx2.files_download(path=\"/Inputdata.txt\")\n",
    "#     f.write(res.content)\n",
    "#     f.write(res2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Translate date of the tweet to quarter reference\n",
    "def Quarterlookup(date):\n",
    "    if date>=DT.datetime(2018,10,1):\n",
    "        return '18Q4'\n",
    "    elif date>=DT.datetime(2018,7,1):\n",
    "        return '18Q3'\n",
    "    elif date>=DT.datetime(2018,4,1):\n",
    "        return '18Q2'\n",
    "    elif date>=DT.datetime(2018,1,1):\n",
    "        return '18Q1'\n",
    "    elif date>=DT.datetime(2017,10,1):\n",
    "        return '17Q4'\n",
    "    elif date>=DT.datetime(2017,7,1):\n",
    "        return '17Q3'\n",
    "    elif date>=DT.datetime(2017,4,1):\n",
    "        return '17Q2'\n",
    "    elif date>=DT.datetime(2017,1,1):\n",
    "        return '17Q1'\n",
    "    elif date>=DT.datetime(2016,10,1):\n",
    "        return '16Q4'\n",
    "    elif date>=DT.datetime(2016,7,1):\n",
    "        return '16Q3'\n",
    "    elif date>=DT.datetime(2016,4,1):\n",
    "        return '16Q2'\n",
    "    elif date>=DT.datetime(2016,1,1):\n",
    "        return '16Q1'\n",
    "    elif date>=DT.datetime(2015,10,1):\n",
    "        return '15Q4'\n",
    "    elif date>=DT.datetime(2015,7,1):\n",
    "        return '15Q3'\n",
    "    elif date>=DT.datetime(2015,4,1):\n",
    "        return '15Q2'\n",
    "    else:\n",
    "        return '15Q1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reading the Input X file and process it\n",
    "d=defaultdict(list)\n",
    "\n",
    "with open('InputX.txt') as f:\n",
    "    for line in f:\n",
    "        count=0\n",
    "        temp =-1\n",
    "        k,v=line.split(':',1)\n",
    "        while True:\n",
    "            \n",
    "            oldtemp=temp\n",
    "            temp=v.find(']]',temp+1)\n",
    "            if count==0:\n",
    "                d[k].append(v[oldtemp+2:temp+2]) \n",
    "            else:\n",
    "                d[k].append(v[oldtemp+4:temp+2])\n",
    "           \n",
    "             \n",
    "            count+=1\n",
    "            if temp == -1: break\n",
    "#             if count == 500: break  # initially limit for time sake\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read from the vocab file pre prepared\n",
    "allword=set()\n",
    "with open('vocab1.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        allword.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5,855\n"
     ]
    }
   ],
   "source": [
    "allword=sorted(allword)\n",
    "vocab = vocabulary.Vocabulary(allword, size=None)  # size=None means unlimited\n",
    "print(\"Vocabulary size: {:,}\".format(vocab.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurrence_matrix(token_ids, V, K=1):\n",
    "    # We'll use this as an \"accumulator\" matrix\n",
    "    C = scipy.sparse.csc_matrix((V,V), dtype=np.float32)\n",
    "    for k in range(1, K+1):\n",
    "\n",
    "       \n",
    "        i = token_ids[:-k]  # current word\n",
    "        j = token_ids[k:]   # k words ahead\n",
    " \n",
    "        data = (np.ones_like(i), (i,j))  # values, indices\n",
    "  \n",
    "        Ck_plus = scipy.sparse.csc_matrix(data, shape=C.shape, dtype=np.float32)\n",
    "        Ck_minus = Ck_plus.T  # Consider k words behind\n",
    "        C += Ck_plus + Ck_minus\n",
    "    \n",
    "\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPMI(C):\n",
    "    \"\"\"Tranform a counts matrix to PPMI.\n",
    "    \n",
    "    Args:\n",
    "      C: scipy.sparse.csc_matrix of counts C_ij\n",
    "    \n",
    "    Returns:\n",
    "      (scipy.sparse.csc_matrix) PPMI(C) as defined above\n",
    "    \"\"\"\n",
    "    Z = float(C.sum())  # total counts\n",
    "    # sum each column (along rows)\n",
    "    Zc = np.array(C.sum(axis=0), dtype=np.float64).flatten()\n",
    "    # sum each row (along columns)\n",
    "    Zr = np.array(C.sum(axis=1), dtype=np.float64).flatten()\n",
    "    \n",
    "    # Get indices of relevant elements\n",
    "    ii, jj = C.nonzero()  # row, column indices\n",
    "    Cij = np.array(C[ii,jj], dtype=np.float64).flatten()\n",
    "    \n",
    "    ##\n",
    "    # PMI equation\n",
    "    pmi = np.log(Cij * Z / (Zr[ii] * Zc[jj]))\n",
    "    ##\n",
    "    # Truncate to positive only\n",
    "    ppmi = np.maximum(0, pmi)  # take positive only\n",
    "    \n",
    "    # Re-format as sparse matrix\n",
    "    ret = scipy.sparse.csc_matrix((ppmi, (ii,jj)), shape=C.shape,\n",
    "                                  dtype=np.float64)\n",
    "    ret.eliminate_zeros()  # remove zeros\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "def SVD(X, d=100):\n",
    "    \"\"\"Returns word vectors from SVD.\n",
    "    \n",
    "    Args:\n",
    "      X: m x n matrix\n",
    "      d: word vector dimension\n",
    "      \n",
    "    Returns:\n",
    "      Wv : m x d matrix, each row is a word vector.\n",
    "    \"\"\"\n",
    "    transformer = TruncatedSVD(n_components=d, random_state=1)\n",
    "    Wv = transformer.fit_transform(X)\n",
    "    # Normalize to unit length\n",
    "    Wv = Wv / np.linalg.norm(Wv, axis=1).reshape([-1,1])\n",
    "    return Wv, transformer.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_np_array(example_ids, max_len=50, pad_id=0):\n",
    "\n",
    "    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\n",
    "    ns = np.zeros([len(example_ids)], dtype=np.int32)\n",
    "    cpy_len = min(len(example_ids), max_len)\n",
    "\n",
    "    arr = example_ids[:cpy_len]\n",
    "    ns = cpy_len\n",
    "    return arr, ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary to collect the tweet detail and the length of each tweet\n",
    "InputX=defaultdict(dict)\n",
    "InputN=defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set of all the stocks of interest\n",
    "#trainstock =['CVSHealth', 'RiteAid', \"Conn's\", \"Casey's\", 'Kroger','WeisMarkets', 'HavertyFurniture', \"Kirkland's\", 'Pier1Imports', 'BuildersFirstSource', 'HuttigBuildingProducts', 'LumberLiquidators', 'TransWorldEntertainment', 'MarineMax', 'Medifast', 'SallyBeauty',\"Sotheby's\", 'StarGasPartners', 'SuburbanPropanePartners', 'TitanMachinery', 'TractorSupply', 'TravelCentersofAmerica', 'Big5SportingGoods', \"Dick'sSportingGoods\", 'HibbettSports', 'Build-A-BearWorkshop']\n",
    "\n",
    "trainstock =['RiteAid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Converting the string of data into dictionary\n",
    "\n",
    "\n",
    "for key,value in d.items():\n",
    "    Inputx=defaultdict(list)\n",
    "    Inputn=defaultdict(list)\n",
    "    if key in trainstock:\n",
    "        for value2 in value:\n",
    "        \n",
    "            temp=value2.find('), ')\n",
    "            date=value2[14:temp+3]\n",
    "            try:\n",
    "                year=int(date[1:5])\n",
    "                month=int(date[date.find(',')+2:date.find(',',date.find(',')+1)])\n",
    "                day=int(date[date.find(')')-2:date.find(')')])\n",
    "            \n",
    "                sentence, length = pad_np_array (vocab.words_to_ids(ast.literal_eval(value2[temp+3:-1])))\n",
    "            \n",
    "                Inputx[Quarterlookup(DT.datetime(year,month,day))].append(sentence)\n",
    "                Inputn[Quarterlookup(DT.datetime(year,month,day))].append(length)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    InputX[key]=OrderedDict(sorted(Inputx.items()))\n",
    "    InputN[key]=OrderedDict(sorted(Inputn.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CVSHealth': 'CVS', 'RiteAid': 'RAD', \"Conn's\": 'CONN', \"Casey's\": 'CASY', 'Kroger': 'KR', 'WeisMarkets': 'WMK', 'HavertyFurniture': 'HVT', \"Kirkland's\": 'KIRK', 'Pier1Imports': 'PIR', 'BuildersFirstSource': 'BLDR', 'HuttigBuildingProducts': 'HBP', 'LumberLiquidators': 'LL', 'TransWorldEntertainment': 'TWMC', 'MarineMax': 'HZO', 'Medifast': 'MED', 'SallyBeauty': 'SBH', \"Sotheby's\": 'BID', 'StarGasPartners': 'SGU', 'SuburbanPropanePartners': 'SPH', 'TitanMachinery': 'TITN', 'TractorSupply': 'TSCO', 'TravelCentersofAmerica': 'TA', 'Big5SportingGoods': 'BGFV', \"Dick'sSportingGoods\": 'DKS', 'HibbettSports': 'HIBB', 'Build-A-BearWorkshop': 'BBW'}\n"
     ]
    }
   ],
   "source": [
    "##List of stockname and relevant stock code\n",
    "l = [(\"CVSHealth\", \"CVS\"),\n",
    "(\"RiteAid\", \"RAD\"),\n",
    "(\"Conn's\", \"CONN\"),\n",
    "(\"Casey's\", \"CASY\"),\n",
    "(\"Kroger\", \"KR\"),\n",
    "(\"WeisMarkets\", \"WMK\"),\n",
    "(\"HavertyFurniture\", \"HVT\"),\n",
    "(\"Kirkland's\", \"KIRK\"),\n",
    "(\"Pier1Imports\",\"PIR\"),\n",
    "(\"BuildersFirstSource\",\"BLDR\"),\n",
    "(\"HuttigBuildingProducts\", \"HBP\"),\n",
    "(\"LumberLiquidators\", \"LL\"),\n",
    "(\"TransWorldEntertainment\", \"TWMC\"),\n",
    "(\"MarineMax\", \"HZO\"),\n",
    "(\"Medifast\", \"MED\"),\n",
    "(\"SallyBeauty\", \"SBH\"),\n",
    "(\"Sotheby's\", \"BID\"),\n",
    "(\"StarGasPartners\", \"SGU\"),\n",
    "(\"SuburbanPropanePartners\", \"SPH\"),\n",
    "(\"TitanMachinery\", \"TITN\"),\n",
    "(\"TractorSupply\",\"TSCO\"),\n",
    "(\"TravelCentersofAmerica\", \"TA\"),\n",
    "(\"Big5SportingGoods\", \"BGFV\"),\n",
    "(\"Dick'sSportingGoods\", \"DKS\"),\n",
    "(\"HibbettSports\", \"HIBB\"),\n",
    "(\"Build-A-BearWorkshop\", \"BBW\")]\n",
    "     \n",
    "all_stock = {}\n",
    "[all_stock.update({k:v}) for k,v in l]\n",
    "print(all_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Read the quarterly return detail from edgaronline and convert into library\n",
    "top=\"http://datafied.api.edgar-online.com/v2/corefinancials/qtr?primarysymbols=\"\n",
    "bottom=\"&appkey=asc97xrhkyu4959aptu76zxj\"\n",
    "Outputy =defaultdict(dict)\n",
    "for stockname,stock in all_stock.items():\n",
    "    check=top+stock+bottom\n",
    "    with urllib.request.urlopen(check) as url:\n",
    "        data = json.loads(url.read().decode())\n",
    "    count1=0\n",
    "    count2=0\n",
    "    end_date=[]\n",
    "    revenue_change=[]\n",
    "    last_rev=0\n",
    "\n",
    "    for key, value in data.items():\n",
    "        \n",
    "        for key2, value2 in value.items():          \n",
    "        \n",
    "            if count1!=0:  \n",
    "                for i in reversed(value2):\n",
    "                  \n",
    "                    for j in i[\"values\"]:\n",
    "                     \n",
    "                        if count2!=0:   \n",
    "                            if j[\"field\"] ==\"periodenddate\":\n",
    "                                end_date.append(Quarterlookup(DT.datetime.strptime(j['value'], '%m/%d/%Y')))\n",
    "                       \n",
    "                            ##Convert revenue into the y_label of 0 for negative growth and 1 for positive growth\n",
    "                            if j[\"field\"] ==\"totalrevenue\":\n",
    "                                revenue_cal =round(float(j['value'])/float(last_rev) - 1,3)\n",
    "                                if revenue_cal>0:\n",
    "                                    revenue_change.append(1)\n",
    "                                else:\n",
    "                                    revenue_change.append(0)\n",
    "                        else:\n",
    "                            \n",
    "                            if j[\"field\"] ==\"totalrevenue\":      \n",
    "                                last_rev=j['value']\n",
    "                              \n",
    "                    count2+=1\n",
    "            count1+=1\n",
    "            \n",
    "    ##Output will be a dictionary with stockname\n",
    "    Outputy[stockname][\"period\"]=end_date\n",
    "    Outputy[stockname][\"revenuechange\"]=revenue_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CVSHealth', 'RiteAid', \"Conn's\", \"Casey's\", 'Kroger', 'WeisMarkets', 'HavertyFurniture', \"Kirkland's\", 'Pier1Imports', 'BuildersFirstSource', 'HuttigBuildingProducts', 'LumberLiquidators', 'TransWorldEntertainment', 'MarineMax', 'Medifast', 'SallyBeauty', \"Sotheby's\", 'StarGasPartners', 'SuburbanPropanePartners', 'TitanMachinery', 'TractorSupply', 'TravelCentersofAmerica', 'Big5SportingGoods', \"Dick'sSportingGoods\", 'HibbettSports', 'Build-A-BearWorkshop']\n"
     ]
    }
   ],
   "source": [
    "y_keys = list(Outputy.keys())\n",
    "print(y_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'period': ['15Q1', '15Q2', '15Q3', '15Q4', '16Q1', '16Q2', '16Q3', '16Q4', '17Q1', '17Q2', '17Q3', '17Q4', '18Q1', '18Q2', '18Q3'], 'revenuechange': [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(Outputy[y_keys[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Outputy.txt', 'w') as f:\n",
    "    print(Outputy, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_init = []\n",
    "train_n_init = []\n",
    "train_y_init = []\n",
    "dev_x_init = []\n",
    "dev_n_init = []\n",
    "dev_y_init = []\n",
    "test_x_init = []\n",
    "test_n_init = []\n",
    "test_y_init = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainquarter=['15Q1','15Q2','15Q3','15Q4','16Q1','16Q2','16Q3','16Q4','17Q1','17Q2','17Q3','17Q4','18Q1']\n",
    "\n",
    "for i in trainstock:\n",
    "    for j in range(len(trainquarter)):\n",
    "        try:\n",
    "                train_x_init.append(InputX [i][trainquarter[j]])\n",
    "                train_n_init.append(InputN [i][trainquarter[j]])\n",
    "                length=len(InputN [i][trainquarter[j]])           \n",
    "                train_y_init.append([Outputy [i]['revenuechange'][j]]*length)\n",
    "      \n",
    "        except:\n",
    "            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teststock =['CVSHealth', 'RiteAid', \"Conn's\", \"Casey's\", 'Kroger','WeisMarkets', 'HavertyFurniture', \"Kirkland's\", 'Pier1Imports', 'BuildersFirstSource', 'HuttigBuildingProducts', 'LumberLiquidators', 'TransWorldEntertainment', 'MarineMax', 'Medifast', 'SallyBeauty',\"Sotheby's\", 'StarGasPartners', 'SuburbanPropanePartners', 'TitanMachinery', 'TractorSupply', 'TravelCentersofAmerica', 'Big5SportingGoods', \"Dick'sSportingGoods\", 'HibbettSports', 'Build-A-BearWorkshop']\n",
    "teststock =['RiteAid']\n",
    "testquarter=['18Q2','18Q3']\n",
    "\n",
    "for i in teststock:\n",
    "    for j in range(len(testquarter)):\n",
    "        try:\n",
    "            if testquarter[j]=='18Q3':\n",
    "                test_x_init.append(InputX [i][testquarter[j]])\n",
    "                test_n_init.append(InputN [i][testquarter[j]])\n",
    "                length=len(InputN [i][testquarter[j]])\n",
    "                test_y_init.append([Outputy [i]['revenuechange'][j]]*length)   \n",
    "            \n",
    "            elif testquarter[j]=='18Q2':\n",
    "                dev_x_init.append(InputX [i][testquarter[j]])\n",
    "                dev_n_init.append(InputN [i][testquarter[j]])\n",
    "                length=len(InputN [i][testquarter[j]])\n",
    "                dev_y_init.append([Outputy [i]['revenuechange'][j]]*length)     \n",
    "               \n",
    "      \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_x_init2 = []\n",
    "train_n_init2 = []\n",
    "train_y_init2 = []\n",
    "dev_x_init2 = []\n",
    "dev_n_init2 = []\n",
    "dev_y_init2 = []\n",
    "test_x_init2 = []\n",
    "test_n_init2 = []\n",
    "test_y_init2 = []\n",
    "\n",
    "for i in range(len(train_x_init)):\n",
    "    for j in range(len(train_x_init[i])):\n",
    "        train_x_init2.append(train_x_init[i][j])\n",
    "        train_n_init2.append(train_n_init[i][j])\n",
    "        train_y_init2.append(train_y_init[i][j])\n",
    "        \n",
    "for i in range(len(dev_x_init)):\n",
    "    for j in range(len(dev_x_init[i])):\n",
    "        dev_x_init2.append(dev_x_init[i][j])\n",
    "        dev_n_init2.append(dev_n_init[i][j])\n",
    "        dev_y_init2.append(dev_y_init[i][j])\n",
    "        \n",
    "for i in range(len(test_x_init)):\n",
    "    for j in range(len(test_x_init[i])):\n",
    "        test_x_init2.append(test_x_init[i][j])\n",
    "        test_n_init2.append(test_n_init[i][j])\n",
    "        test_y_init2.append(test_y_init[i][j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL BAG OF WORDS MODEL #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2763, 169, 3496, 5259, 2937, 5259, 2157, 169, 5778, 873, 2350, 5453, 4366, 3630, 2183, 847, 2995, 3603, 4476, 5196, 4203, 169, 3603, 5778, 873, 1537, 5260, 169, 2350, 5453, 5102, 3630, 5557, 1653, 5259, 5201, 2608, 3425, 169, 2466, 169, 64, 668, 169, 4509, 169, 4509, 169, 2848, 169]\n",
      "[3440, 246, 5259, 3378, 2407, 1067, 169, 2481, 4831, 5174, 5444, 1998, 204, 3235, 169, 169, 1608, 473, 169, 4509, 169, 4500, 3566, 12, 169, 4509, 169, 169, 4081]\n",
      "[3440, 246, 5259, 3378, 2407, 1067, 169, 2481, 4831, 5174, 5444, 1998, 204, 3235, 169, 169, 1608, 473, 169, 4509, 169, 4500, 3566, 12, 169, 4509, 169, 169, 4081]\n",
      "[2830, 1689, 882, 2756, 558, 169, 169, 2859, 169, 2787, 169, 2876, 169, 2788, 169, 5183, 169, 2869, 169, 3069, 169, 5723, 169, 1061, 169, 2174, 169, 5551, 169, 882, 169, 1689, 169, 2849, 169, 4509, 169, 4835, 169, 1690, 169, 2112, 3810]\n",
      "[4508, 294, 169, 169, 169, 169, 2046, 4503, 1176, 169, 1234, 169, 169, 1347, 3566, 2148, 897, 12, 2971, 169, 2927, 169, 1160, 12, 871, 169, 4891, 12, 882, 570, 12, 169, 3332, 52, 2139, 5164, 239, 12, 1347, 12, 169, 1198, 169, 2465, 169, 66, 2579, 110, 5851, 169]\n",
      "[626, 4508, 294, 3262, 5632, 1347, 169, 5841, 169, 2466, 169, 64, 2195, 169, 1196, 169, 4509, 169, 4615]\n",
      "[609, 5164, 1338, 1687, 5617, 5590, 5259, 4702, 3632, 204, 646, 5156, 5778, 5259, 1764, 5675, 1538, 12, 2994, 12, 4740, 12, 1090, 12, 3603, 4475, 401, 3548, 3630, 4208, 5770, 169, 169, 2164, 169, 4509, 169, 2848, 169, 3228, 169, 5711, 169, 2156, 3909]\n",
      "[626, 4508, 294, 3262, 5632, 1347, 169, 5841, 169, 2466, 169, 64, 2195, 169, 1196, 169, 4509, 169, 4615]\n",
      "[5681, 5735, 5778, 4203, 4861, 2993, 5196, 169, 5196, 2756, 204, 2445, 2596, 169, 1728, 2608, 2989, 3548, 4391, 1760, 169, 4509, 169, 5513, 169, 3702, 169, 3234]\n",
      "[2830, 1689, 882, 2756, 558, 169, 169, 2859, 169, 2787, 169, 2876, 169, 2788, 169, 5183, 169, 2869, 169, 3069, 169, 5723, 169, 1061, 169, 2174, 169, 5551, 169, 882, 169, 1689, 169, 2849, 169, 4509, 169, 4835, 3949]\n",
      "[5681, 5735, 5778, 4203, 4861, 2993, 5196, 169, 5196, 2756, 204, 2445, 2596, 169, 1728, 2608, 2989, 3548, 4391, 1760, 169, 4509, 169, 5513, 169, 3702, 169, 3234]\n",
      "[4544, 169, 2831, 169, 3565, 2830, 1689, 882, 2756, 4836, 558, 169, 2139, 2830, 473, 4508, 294, 3769, 169, 2859, 169, 2787, 169, 2876, 169, 2788, 169, 5183, 169, 2869, 169, 3069, 169, 5723, 169, 1061, 169, 2174, 169, 5551, 169, 882, 169, 1689, 169, 2849, 169, 4509, 169, 4835]\n",
      "[2830, 1689, 882, 2756, 558, 169, 169, 2859, 169, 2787, 169, 2876, 169, 2788, 169, 5183, 169, 2869, 169, 3069, 169, 5723, 169, 1061, 169, 2174, 169, 5551, 169, 882, 169, 1689, 169, 2849, 169, 4509, 169, 4835, 3941]\n",
      "[4508, 294, 169, 169, 169, 169, 2046, 4503, 1176, 169, 1234, 169, 169, 1347, 3566, 2148, 897, 12, 2971, 169, 2927, 169, 1160, 12, 871, 169, 4891, 12, 882, 570, 12, 169, 3332, 52, 2139, 5164, 239, 12, 1347, 12, 169, 1198, 169, 2465, 169, 66, 2579, 110, 5851, 169]\n",
      "[2830, 1689, 882, 2756, 558, 169, 169, 2859, 169, 2787, 169, 2876, 169, 2788, 169, 5183, 169, 2869, 169, 3069, 169, 5723, 169, 1061, 169, 2174, 169, 5551, 169, 882, 169, 1689, 169, 2849, 169, 4509, 169, 4835, 3992]\n",
      "[3440, 246, 5259, 3378, 2407, 1067, 169, 2481, 4831, 5174, 5444, 1998, 204, 3235, 169, 169, 1608, 473, 169, 4509, 169, 4500, 3566, 12, 169, 4509, 169, 169, 4081]\n",
      "[2830, 1689, 882, 2756, 558, 169, 169, 2859, 169, 2787, 169, 2876, 169, 2788, 169, 5183, 169, 2869, 169, 3069, 169, 5723, 169, 1061, 169, 2174, 169, 5551, 169, 882, 169, 1689, 169, 2849, 169, 4509, 169, 4835, 3949]\n",
      "[4508, 294, 169, 169, 169, 169, 2046, 4503, 1176, 169, 1234, 169, 169, 1347, 3566, 2148, 897, 12, 2971, 169, 2927, 169, 1160, 12, 871, 169, 4891, 12, 882, 570, 12, 169, 3332, 52, 2139, 5164, 239, 12, 1347, 12, 169, 1198, 169, 2465, 169, 66, 2579, 110, 5851, 169]\n",
      "[626, 4508, 294, 3262, 5632, 1347, 169, 5841, 169, 2466, 169, 64, 2195, 169, 1196, 169, 4509, 169, 4615]\n",
      "[169, 2785, 4401, 2329, 1468, 1707, 1556, 4741, 169, 4509, 169, 4341, 169, 353]\n",
      "[169, 4509, 169, 4508, 294, 2466, 169, 66, 2720, 762, 169, 5488, 169, 2564, 5851]\n",
      "[3565, 2830, 1689, 882, 2756, 4836, 558, 169, 2139, 2830, 473, 4508, 294, 3769, 169, 2859, 169, 2787, 169, 2876, 169, 2788, 169, 5183, 169, 2869, 169, 3069, 169, 5723, 169, 1061, 169, 2174, 169, 5551, 169, 882, 169, 1689, 169, 2849, 169, 4509, 169, 4835, 169, 5775, 3892]\n",
      "[2830, 1689, 882, 2756, 558, 169, 169, 2859, 169, 2787, 169, 2876, 169, 2788, 169, 5183, 169, 2869, 169, 3069, 169, 5723, 169, 1061, 169, 2174, 169, 5551, 169, 882, 169, 1689, 169, 2849, 169, 4509, 169, 4835, 3949]\n",
      "[169, 2785, 4401, 2329, 1468, 1707, 1556, 4741, 169, 4509, 169, 4341, 169, 353]\n",
      "[5681, 5735, 5778, 4203, 4861, 2993, 5196, 169, 5196, 2756, 204, 2445, 2596, 169, 1728, 2608, 2989, 3548, 4391, 1760, 169, 4509, 169, 5513, 169, 3702, 169, 3234]\n",
      "[3440, 246, 5259, 3378, 2407, 1067, 169, 2481, 4831, 5174, 5444, 1998, 204, 3235, 169, 169, 1608, 473, 169, 4509, 169, 4500, 3566, 12, 169, 4509, 169, 169, 4081]\n",
      "[169, 4509, 169, 4508, 294, 2466, 169, 66, 2720, 762, 169, 5488, 169, 2564, 5851]\n",
      "[2830, 1689, 882, 2756, 558, 169, 169, 2859, 169, 2787, 169, 2876, 169, 2788, 169, 5183, 169, 2869, 169, 3069, 169, 5723, 169, 1061, 169, 2174, 169, 5551, 169, 882, 169, 1689, 169, 2849, 169, 4509, 169, 4835, 3992]\n",
      "[4508, 294, 169, 169, 169, 169, 2046, 4503, 1176, 169, 1234, 169, 169, 1347, 3566, 2148, 897, 12, 2971, 169, 2927, 169, 1160, 12, 871, 169, 4891, 12, 882, 570, 12, 169, 3332, 52, 2139, 5164, 239, 12, 1347, 12, 169, 1198, 169, 2465, 169, 66, 2579, 110, 5851, 169]\n",
      "[2763, 169, 3496, 5259, 2937, 5259, 2157, 169, 5778, 873, 2350, 5453, 4366, 3630, 2183, 847, 2995, 3603, 4476, 5196, 4203, 169, 3603, 5778, 873, 1537, 5260, 169, 2350, 5453, 5102, 3630, 5557, 1653, 5259, 5201, 2608, 3425, 169, 2466, 169, 64, 668, 169, 4509, 169, 4509, 169, 2848, 169]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\n",
    "    print(train_x_init2[i])\n",
    "    print(test_x_init2[i])\n",
    "    print(dev_x_init2[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_np_array2(example_ids, max_len=50, pad_id=0):\n",
    "\n",
    "    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\n",
    "    ns = np.zeros([len(example_ids)], dtype=np.int32)\n",
    "    for i, ids in enumerate(example_ids):\n",
    "        cpy_len = min(len(ids), max_len)\n",
    "        arr[i,:cpy_len] = ids[:cpy_len]\n",
    "        ns[i] = cpy_len\n",
    "    return arr ##I HAD TO REMOVE NS TO MAKE IT WORK< PLEASE TAKE A LOOK IF IT HAS ANY IMPACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pad_np_array2(train_x_init2, max_len = 50, pad_id=0)\n",
    "train_n = np.array(train_n_init2)\n",
    "train_y = np.array(train_y_init2)\n",
    "\n",
    "test_x = pad_np_array2(test_x_init2, max_len = 50, pad_id=0)\n",
    "test_n = np.array(test_n_init2)\n",
    "test_y = np.array(test_y_init2)\n",
    "\n",
    "dev_x = pad_np_array2(dev_x_init2, max_len = 50, pad_id=0)\n",
    "dev_n = np.array(dev_n_init2)\n",
    "dev_y = np.array(dev_y_init2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214320\n",
      "214320\n",
      "214320\n",
      "16372\n",
      "16372\n",
      "16372\n",
      "16608\n",
      "16608\n",
      "16608\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x))\n",
    "print(len(train_n))\n",
    "print(len(train_y))\n",
    "print(len(dev_x))\n",
    "print(len(dev_n))\n",
    "print(len(dev_y))\n",
    "print(len(test_x))\n",
    "print(len(test_n))\n",
    "print(len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 16608)]\n",
      "[0]\n",
      "[16608]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-f9d019a2bc8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# counts =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlbl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcnts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlbl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "lbl, cnts = np.unique(test_y, return_counts=True)\n",
    "print(list(zip(lbl, cnts)))\n",
    "print(lbl)\n",
    "print(cnts)\n",
    "\n",
    "# labels = \n",
    "# counts = \n",
    "\n",
    "plt.hist((cnts[0]*[lbl[0]],cnts[1]*[lbl[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl2, cnts2 = np.unique(train_y, return_counts=True)\n",
    "print(list(zip(lbl2, cnts2)))\n",
    "print(lbl2[0])\n",
    "print(cnts2[0])\n",
    "\n",
    "# labels = \n",
    "# counts = \n",
    "\n",
    "plt.hist((cnts2[0]*[lbl2[0]],cnts2[1]*[lbl2[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15,625 examples, moving-average loss 0.20\n",
      "31,250 examples, moving-average loss 0.13\n",
      "46,875 examples, moving-average loss 0.09\n",
      "62,500 examples, moving-average loss 0.07\n",
      "78,125 examples, moving-average loss 0.08\n",
      "93,750 examples, moving-average loss 0.07\n",
      "109,375 examples, moving-average loss 0.06\n",
      "125,000 examples, moving-average loss 0.08\n",
      "140,625 examples, moving-average loss 0.07\n",
      "156,250 examples, moving-average loss 0.07\n",
      "171,875 examples, moving-average loss 0.06\n",
      "187,500 examples, moving-average loss 0.06\n",
      "203,125 examples, moving-average loss 0.05\n",
      "Completed one epoch in 0:00:10\n"
     ]
    }
   ],
   "source": [
    "reload(models)\n",
    "\n",
    "x, ns, y = train_x, train_n, train_y\n",
    "batch_size = 125\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=vocab.size, embed_dim=50, hidden_dims=[25], num_classes=2,\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "model_fn = models.classifier_model_fn\n",
    "\n",
    "total_batches = 0\n",
    "total_examples = 0\n",
    "total_loss = 0\n",
    "loss_ema = np.log(2)  # track exponential-moving-average of loss\n",
    "ema_decay = np.exp(-1/10)  # decay parameter for moving average = np.exp(-1/history_length)\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    ##\n",
    "    # Construct the graph here. No session.run calls - just wiring up Tensors.\n",
    "    ##\n",
    "    # Add placeholders so we can feed in data.\n",
    "    x_ph_  = tf.placeholder(tf.int32, shape=[None, x.shape[1]])  # [batch_size, max_len]\n",
    "    ns_ph_ = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    y_ph_  = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    \n",
    "    # Construct the graph using model_fn\n",
    "    features = {\"ids\": x_ph_, \"ns\": ns_ph_}  # note that values are Tensors\n",
    "    estimator_spec = model_fn(features, labels=y_ph_, mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                              params=model_params)\n",
    "    loss_     = estimator_spec.loss\n",
    "    train_op_ = estimator_spec.train_op\n",
    "    \n",
    "    ##\n",
    "    # Done constructing the graph, now we can make session.run calls.\n",
    "    ##\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Run a single epoch\n",
    "    t0 = time.time()\n",
    "    for (bx, bns, by) in utils.multi_batch_generator(batch_size, x, ns, y):\n",
    "        # feed NumPy arrays into the placeholder Tensors\n",
    "        feed_dict = {x_ph_: bx, ns_ph_: bns, y_ph_: by}\n",
    "        batch_loss, _ = sess.run([loss_, train_op_], feed_dict=feed_dict)\n",
    "        \n",
    "        # Compute some statistics\n",
    "        total_batches += 1\n",
    "        total_examples += len(bx)\n",
    "        total_loss += batch_loss * len(bx)  # re-scale, since batch loss is mean\n",
    "        # Compute moving average to smooth out noisy per-batch loss\n",
    "        loss_ema = ema_decay * loss_ema + (1 - ema_decay) * batch_loss\n",
    "        \n",
    "        if (total_batches % 125 == 0):\n",
    "            print(\"{:5,} examples, moving-average loss {:.2f}\".format(total_examples, \n",
    "                                                                      loss_ema))    \n",
    "    print(\"Completed one epoch in {:s}\".format(utils.pretty_timedelta(since=t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (5,855 words) written to '/tmp/tf_bow_sst_20181203-1844/metadata.tsv'\n",
      "Projector config written to /tmp/tf_bow_sst_20181203-1844/projector_config.pbtxt\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tf_bow_sst_20181203-1844', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9c7718e630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    tensorboard --logdir='/tmp/tf_bow_sst_20181203-1844' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "import models; reload(models)\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=vocab.size, embed_dim=50, hidden_dims=[25], num_classes=2,\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "\n",
    "checkpoint_dir = \"/tmp/tf_bow_sst_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "# Write vocabulary to file, so TensorBoard can label embeddings.\n",
    "# creates checkpoint_dir/projector_config.pbtxt and checkpoint_dir/metadata.tsv\n",
    "vocab.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=models.classifier_model_fn, \n",
    "                               params=model_params,\n",
    "                               model_dir=checkpoint_dir)\n",
    "print(\"\")\n",
    "print(\"To view training (once it starts), run:\\n\")\n",
    "print(\"    tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "print(\"\\nThen in your browser, open: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tf_bow_sst_20181203-1844/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1960714, step = 1\n",
      "INFO:tensorflow:global_step/sec: 131.023\n",
      "INFO:tensorflow:loss = 0.28880808, step = 101 (0.766 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.037\n",
      "INFO:tensorflow:loss = 0.17751586, step = 201 (0.709 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.911\n",
      "INFO:tensorflow:loss = 0.11779082, step = 301 (0.709 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.973\n",
      "INFO:tensorflow:loss = 0.17678559, step = 401 (0.695 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.34\n",
      "INFO:tensorflow:loss = 0.85279834, step = 501 (0.712 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.174\n",
      "INFO:tensorflow:loss = 0.095653996, step = 601 (0.714 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.097\n",
      "INFO:tensorflow:loss = 0.086235724, step = 701 (0.713 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.773\n",
      "INFO:tensorflow:loss = 0.070414655, step = 801 (0.711 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.928\n",
      "INFO:tensorflow:loss = 0.8343995, step = 901 (0.714 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.988\n",
      "INFO:tensorflow:loss = 0.0843198, step = 1001 (0.685 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.245\n",
      "INFO:tensorflow:loss = 0.07412812, step = 1101 (0.723 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.485\n",
      "INFO:tensorflow:loss = 0.06298066, step = 1201 (0.707 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.135\n",
      "INFO:tensorflow:loss = 0.6000354, step = 1301 (0.724 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.435\n",
      "INFO:tensorflow:loss = 0.07854596, step = 1401 (0.708 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.33\n",
      "INFO:tensorflow:loss = 0.063834995, step = 1501 (0.703 sec)\n",
      "INFO:tensorflow:global_step/sec: 137.666\n",
      "INFO:tensorflow:loss = 0.060652737, step = 1601 (0.725 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.057\n",
      "INFO:tensorflow:loss = 0.058616556, step = 1701 (0.710 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.863\n",
      "INFO:tensorflow:loss = 0.057130367, step = 1801 (0.710 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.761\n",
      "INFO:tensorflow:loss = 0.053472433, step = 1901 (0.721 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.139\n",
      "INFO:tensorflow:loss = 0.05131575, step = 2001 (0.699 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.725\n",
      "INFO:tensorflow:loss = 0.314914, step = 2101 (0.700 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.409\n",
      "INFO:tensorflow:loss = 0.078384615, step = 2201 (0.724 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.88\n",
      "INFO:tensorflow:loss = 0.059523303, step = 2301 (0.698 sec)\n",
      "INFO:tensorflow:global_step/sec: 146.533\n",
      "INFO:tensorflow:loss = 0.08617802, step = 2401 (0.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.992\n",
      "INFO:tensorflow:loss = 0.07005627, step = 2501 (0.703 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.194\n",
      "INFO:tensorflow:loss = 0.06621764, step = 2601 (0.693 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.147\n",
      "INFO:tensorflow:loss = 0.07026144, step = 2701 (0.704 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.066\n",
      "INFO:tensorflow:loss = 0.0754328, step = 2801 (0.694 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.178\n",
      "INFO:tensorflow:loss = 0.06532799, step = 2901 (0.712 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.277\n",
      "INFO:tensorflow:loss = 0.061634794, step = 3001 (0.724 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.891\n",
      "INFO:tensorflow:loss = 0.07361001, step = 3101 (0.704 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.673\n",
      "INFO:tensorflow:loss = 0.06295996, step = 3201 (0.712 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.354\n",
      "INFO:tensorflow:loss = 0.058846347, step = 3301 (0.723 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.354\n",
      "INFO:tensorflow:loss = 0.057604447, step = 3401 (0.703 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3430 into /tmp/tf_bow_sst_20181203-1844/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.045663588.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-03-18:45:22\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181203-1844/model.ckpt-3430\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-03-18:45:22\n",
      "INFO:tensorflow:Saving dict for global step 3430: accuracy = 0.0, cross_entropy_loss = 4.9656973, global_step = 3430, loss = 5.0137215\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3430: /tmp/tf_bow_sst_20181203-1844/model.ckpt-3430\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181203-1844/model.ckpt-3430\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3430 into /tmp/tf_bow_sst_20181203-1844/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.05542266, step = 3431\n",
      "INFO:tensorflow:global_step/sec: 127.755\n",
      "INFO:tensorflow:loss = 0.054257125, step = 3531 (0.785 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.921\n",
      "INFO:tensorflow:loss = 0.052002165, step = 3631 (0.686 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.454\n",
      "INFO:tensorflow:loss = 0.050046608, step = 3731 (0.701 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.763\n",
      "INFO:tensorflow:loss = 0.15015405, step = 3831 (0.716 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.986\n",
      "INFO:tensorflow:loss = 1.101273, step = 3931 (0.694 sec)\n",
      "INFO:tensorflow:global_step/sec: 136.546\n",
      "INFO:tensorflow:loss = 0.086965054, step = 4031 (0.732 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.637\n",
      "INFO:tensorflow:loss = 0.08075071, step = 4131 (0.738 sec)\n",
      "INFO:tensorflow:global_step/sec: 132.342\n",
      "INFO:tensorflow:loss = 0.0689331, step = 4231 (0.756 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.571\n",
      "INFO:tensorflow:loss = 1.3164976, step = 4331 (0.957 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.709\n",
      "INFO:tensorflow:loss = 0.082347095, step = 4431 (0.984 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.7217\n",
      "INFO:tensorflow:loss = 0.07059415, step = 4531 (1.012 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.258\n",
      "INFO:tensorflow:loss = 0.06351374, step = 4631 (0.924 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.998\n",
      "INFO:tensorflow:loss = 0.580098, step = 4731 (0.942 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.893\n",
      "INFO:tensorflow:loss = 0.073995896, step = 4831 (0.927 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.065\n",
      "INFO:tensorflow:loss = 0.060416464, step = 4931 (0.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.228\n",
      "INFO:tensorflow:loss = 0.05825676, step = 5031 (0.917 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.754\n",
      "INFO:tensorflow:loss = 0.057060692, step = 5131 (0.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.893\n",
      "INFO:tensorflow:loss = 0.056237433, step = 5231 (0.954 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.692\n",
      "INFO:tensorflow:loss = 0.053013623, step = 5331 (0.946 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.321\n",
      "INFO:tensorflow:loss = 0.051240228, step = 5431 (0.923 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.65\n",
      "INFO:tensorflow:loss = 0.3007444, step = 5531 (0.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.624\n",
      "INFO:tensorflow:loss = 0.07841947, step = 5631 (0.764 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.789\n",
      "INFO:tensorflow:loss = 0.057405785, step = 5731 (0.706 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.097\n",
      "INFO:tensorflow:loss = 0.08447678, step = 5831 (0.698 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.143\n",
      "INFO:tensorflow:loss = 0.070761256, step = 5931 (0.715 sec)\n",
      "INFO:tensorflow:global_step/sec: 134.797\n",
      "INFO:tensorflow:loss = 0.067496665, step = 6031 (0.742 sec)\n",
      "INFO:tensorflow:global_step/sec: 137.407\n",
      "INFO:tensorflow:loss = 0.06777632, step = 6131 (0.727 sec)\n",
      "INFO:tensorflow:global_step/sec: 137.384\n",
      "INFO:tensorflow:loss = 0.07433117, step = 6231 (0.728 sec)\n",
      "INFO:tensorflow:global_step/sec: 136.648\n",
      "INFO:tensorflow:loss = 0.06559163, step = 6331 (0.732 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.419\n",
      "INFO:tensorflow:loss = 0.062378686, step = 6431 (0.713 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.657\n",
      "INFO:tensorflow:loss = 0.07208417, step = 6531 (0.700 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.486\n",
      "INFO:tensorflow:loss = 0.062033504, step = 6631 (0.712 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 142.049\n",
      "INFO:tensorflow:loss = 0.05826583, step = 6731 (0.705 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.638\n",
      "INFO:tensorflow:loss = 0.05732845, step = 6831 (0.702 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6860 into /tmp/tf_bow_sst_20181203-1844/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.045052115.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-03-18:45:51\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181203-1844/model.ckpt-6860\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-03-18:45:52\n",
      "INFO:tensorflow:Saving dict for global step 6860: accuracy = 0.0, cross_entropy_loss = 4.9667664, global_step = 6860, loss = 5.014501\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6860: /tmp/tf_bow_sst_20181203-1844/model.ckpt-6860\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181203-1844/model.ckpt-6860\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 6860 into /tmp/tf_bow_sst_20181203-1844/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.055157233, step = 6861\n",
      "INFO:tensorflow:global_step/sec: 128.62\n",
      "INFO:tensorflow:loss = 0.054280963, step = 6961 (0.780 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.296\n",
      "INFO:tensorflow:loss = 0.052181125, step = 7061 (0.717 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.938\n",
      "INFO:tensorflow:loss = 0.050353047, step = 7161 (0.706 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.627\n",
      "INFO:tensorflow:loss = 0.14678855, step = 7261 (0.706 sec)\n",
      "INFO:tensorflow:global_step/sec: 137.892\n",
      "INFO:tensorflow:loss = 1.3725979, step = 7361 (0.725 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.145\n",
      "INFO:tensorflow:loss = 0.08786681, step = 7461 (0.704 sec)\n",
      "INFO:tensorflow:global_step/sec: 137.817\n",
      "INFO:tensorflow:loss = 0.08045177, step = 7561 (0.725 sec)\n",
      "INFO:tensorflow:global_step/sec: 137.792\n",
      "INFO:tensorflow:loss = 0.0701513, step = 7661 (0.725 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.627\n",
      "INFO:tensorflow:loss = 1.3617619, step = 7761 (0.697 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.745\n",
      "INFO:tensorflow:loss = 0.08215375, step = 7861 (0.715 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.785\n",
      "INFO:tensorflow:loss = 0.070918016, step = 7961 (0.721 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.409\n",
      "INFO:tensorflow:loss = 0.06487152, step = 8061 (0.707 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.832\n",
      "INFO:tensorflow:loss = 0.57888603, step = 8161 (0.715 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.125\n",
      "INFO:tensorflow:loss = 0.07318795, step = 8261 (0.709 sec)\n",
      "INFO:tensorflow:global_step/sec: 136.35\n",
      "INFO:tensorflow:loss = 0.059920363, step = 8361 (0.733 sec)\n",
      "INFO:tensorflow:global_step/sec: 137.094\n",
      "INFO:tensorflow:loss = 0.058084026, step = 8461 (0.729 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.642\n",
      "INFO:tensorflow:loss = 0.05714863, step = 8561 (0.697 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.915\n",
      "INFO:tensorflow:loss = 0.056558073, step = 8661 (0.714 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.569\n",
      "INFO:tensorflow:loss = 0.05341582, step = 8761 (0.706 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.995\n",
      "INFO:tensorflow:loss = 0.05174181, step = 8861 (0.720 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.436\n",
      "INFO:tensorflow:loss = 0.29733837, step = 8961 (0.708 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.735\n",
      "INFO:tensorflow:loss = 0.078479305, step = 9061 (0.715 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.827\n",
      "INFO:tensorflow:loss = 0.05702913, step = 9161 (0.695 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.126\n",
      "INFO:tensorflow:loss = 0.0829006, step = 9261 (0.710 sec)\n",
      "INFO:tensorflow:global_step/sec: 137.704\n",
      "INFO:tensorflow:loss = 0.07097219, step = 9361 (0.725 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.493\n",
      "INFO:tensorflow:loss = 0.06808435, step = 9461 (0.702 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.629\n",
      "INFO:tensorflow:loss = 0.06690847, step = 9561 (0.702 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.853\n",
      "INFO:tensorflow:loss = 0.07409649, step = 9661 (0.714 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.469\n",
      "INFO:tensorflow:loss = 0.06616459, step = 9761 (0.702 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.083\n",
      "INFO:tensorflow:loss = 0.063207, step = 9861 (0.705 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.177\n",
      "INFO:tensorflow:loss = 0.071045525, step = 9961 (0.717 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.553\n",
      "INFO:tensorflow:loss = 0.062271673, step = 10061 (0.717 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.982\n",
      "INFO:tensorflow:loss = 0.058825538, step = 10161 (0.719 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.417\n",
      "INFO:tensorflow:loss = 0.058090873, step = 10261 (0.717 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10290 into /tmp/tf_bow_sst_20181203-1844/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.045309033.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-03-18:46:19\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181203-1844/model.ckpt-10290\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-03-18:46:20\n",
      "INFO:tensorflow:Saving dict for global step 10290: accuracy = 0.0, cross_entropy_loss = 4.9675384, global_step = 10290, loss = 5.01599\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10290: /tmp/tf_bow_sst_20181203-1844/model.ckpt-10290\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181203-1844/model.ckpt-10290\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 10290 into /tmp/tf_bow_sst_20181203-1844/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.055877775, step = 10291\n",
      "INFO:tensorflow:global_step/sec: 131.413\n",
      "INFO:tensorflow:loss = 0.05517279, step = 10391 (0.763 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.812\n",
      "INFO:tensorflow:loss = 0.053102225, step = 10491 (0.695 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.197\n",
      "INFO:tensorflow:loss = 0.051286634, step = 10591 (0.693 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.446\n",
      "INFO:tensorflow:loss = 0.14101657, step = 10691 (0.697 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.302\n",
      "INFO:tensorflow:loss = 0.94763446, step = 10791 (0.699 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.876\n",
      "INFO:tensorflow:loss = 0.08748561, step = 10891 (0.719 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.417\n",
      "INFO:tensorflow:loss = 0.07953245, step = 10991 (0.723 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.765\n",
      "INFO:tensorflow:loss = 0.070119925, step = 11091 (0.705 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.263\n",
      "INFO:tensorflow:loss = 1.1268326, step = 11191 (0.718 sec)\n",
      "INFO:tensorflow:global_step/sec: 137.684\n",
      "INFO:tensorflow:loss = 0.08252017, step = 11291 (0.727 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.373\n",
      "INFO:tensorflow:loss = 0.070176266, step = 11391 (0.702 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.408\n",
      "INFO:tensorflow:loss = 0.0650763, step = 11491 (0.707 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.512\n",
      "INFO:tensorflow:loss = 0.6052363, step = 11591 (0.738 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.401\n",
      "INFO:tensorflow:loss = 0.07362397, step = 11691 (0.697 sec)\n",
      "INFO:tensorflow:global_step/sec: 137.079\n",
      "INFO:tensorflow:loss = 0.059818268, step = 11791 (0.729 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.927\n",
      "INFO:tensorflow:loss = 0.058192134, step = 11891 (0.710 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.208\n",
      "INFO:tensorflow:loss = 0.05745192, step = 11991 (0.707 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.391\n",
      "INFO:tensorflow:loss = 0.057048466, step = 12091 (0.724 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.647\n",
      "INFO:tensorflow:loss = 0.05396636, step = 12191 (0.710 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.669\n",
      "INFO:tensorflow:loss = 0.052366763, step = 12291 (0.705 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.611\n",
      "INFO:tensorflow:loss = 0.29589745, step = 12391 (0.706 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 141.801\n",
      "INFO:tensorflow:loss = 0.07804706, step = 12491 (0.706 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.957\n",
      "INFO:tensorflow:loss = 0.057025738, step = 12591 (0.704 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.747\n",
      "INFO:tensorflow:loss = 0.08086192, step = 12691 (0.715 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.205\n",
      "INFO:tensorflow:loss = 0.07082445, step = 12791 (0.700 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.579\n",
      "INFO:tensorflow:loss = 0.0683239, step = 12891 (0.696 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.048\n",
      "INFO:tensorflow:loss = 0.06615277, step = 12991 (0.694 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.362\n",
      "INFO:tensorflow:loss = 0.07297862, step = 13091 (0.713 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.965\n",
      "INFO:tensorflow:loss = 0.06639216, step = 13191 (0.699 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.678\n",
      "INFO:tensorflow:loss = 0.063797675, step = 13291 (0.702 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.732\n",
      "INFO:tensorflow:loss = 0.07020141, step = 13391 (0.705 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.294\n",
      "INFO:tensorflow:loss = 0.062377423, step = 13491 (0.688 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.182\n",
      "INFO:tensorflow:loss = 0.05922463, step = 13591 (0.699 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.922\n",
      "INFO:tensorflow:loss = 0.058678996, step = 13691 (0.704 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 13720 into /tmp/tf_bow_sst_20181203-1844/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.04554098.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-03-18:46:46\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181203-1844/model.ckpt-13720\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-03-18:46:47\n",
      "INFO:tensorflow:Saving dict for global step 13720: accuracy = 0.0, cross_entropy_loss = 4.9648023, global_step = 13720, loss = 5.013798\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 13720: /tmp/tf_bow_sst_20181203-1844/model.ckpt-13720\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181203-1844/model.ckpt-13720\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 13720 into /tmp/tf_bow_sst_20181203-1844/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.056449365, step = 13721\n",
      "INFO:tensorflow:global_step/sec: 126.356\n",
      "INFO:tensorflow:loss = 0.055910923, step = 13821 (0.794 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.683\n",
      "INFO:tensorflow:loss = 0.05389686, step = 13921 (0.709 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.688\n",
      "INFO:tensorflow:loss = 0.052120678, step = 14021 (0.701 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.67\n",
      "INFO:tensorflow:loss = 0.13730288, step = 14121 (0.711 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.908\n",
      "INFO:tensorflow:loss = 0.9025096, step = 14221 (0.714 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.115\n",
      "INFO:tensorflow:loss = 0.08733861, step = 14321 (0.704 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.73\n",
      "INFO:tensorflow:loss = 0.07854, step = 14421 (0.716 sec)\n",
      "INFO:tensorflow:global_step/sec: 133.367\n",
      "INFO:tensorflow:loss = 0.07020724, step = 14521 (0.750 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.463\n",
      "INFO:tensorflow:loss = 1.0536128, step = 14621 (0.711 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.081\n",
      "INFO:tensorflow:loss = 0.082929686, step = 14721 (0.704 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.86\n",
      "INFO:tensorflow:loss = 0.0700075, step = 14821 (0.696 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.42\n",
      "INFO:tensorflow:loss = 0.06560143, step = 14921 (0.721 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.515\n",
      "INFO:tensorflow:loss = 0.5915979, step = 15021 (0.703 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.958\n",
      "INFO:tensorflow:loss = 0.07303955, step = 15121 (0.709 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.141\n",
      "INFO:tensorflow:loss = 0.060477354, step = 15221 (0.698 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.389\n",
      "INFO:tensorflow:loss = 0.05907426, step = 15321 (0.712 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.823\n",
      "INFO:tensorflow:loss = 0.05846634, step = 15421 (0.715 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.406\n",
      "INFO:tensorflow:loss = 0.058205243, step = 15521 (0.717 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.833\n",
      "INFO:tensorflow:loss = 0.05506193, step = 15621 (0.710 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.377\n",
      "INFO:tensorflow:loss = 0.053452637, step = 15721 (0.723 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.582\n",
      "INFO:tensorflow:loss = 0.30250078, step = 15821 (0.705 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.181\n",
      "INFO:tensorflow:loss = 0.07781584, step = 15921 (0.699 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.613\n",
      "INFO:tensorflow:loss = 0.058239967, step = 16021 (0.702 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.665\n",
      "INFO:tensorflow:loss = 0.08357804, step = 16121 (0.715 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.247\n",
      "INFO:tensorflow:loss = 0.0721811, step = 16221 (0.694 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.687\n",
      "INFO:tensorflow:loss = 0.06994813, step = 16321 (0.687 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.065\n",
      "INFO:tensorflow:loss = 0.06714581, step = 16421 (0.703 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.573\n",
      "INFO:tensorflow:loss = 0.077655435, step = 16521 (0.712 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.73\n",
      "INFO:tensorflow:loss = 0.06888602, step = 16621 (0.691 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.821\n",
      "INFO:tensorflow:loss = 0.06619437, step = 16721 (0.715 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.209\n",
      "INFO:tensorflow:loss = 0.07100524, step = 16821 (0.713 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.822\n",
      "INFO:tensorflow:loss = 0.06207198, step = 16921 (0.696 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.704\n",
      "INFO:tensorflow:loss = 0.058983378, step = 17021 (0.686 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.066\n",
      "INFO:tensorflow:loss = 0.05861242, step = 17121 (0.709 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 17150 into /tmp/tf_bow_sst_20181203-1844/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.045241453.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-03-18:47:14\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181203-1844/model.ckpt-17150\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-03-18:47:15\n",
      "INFO:tensorflow:Saving dict for global step 17150: accuracy = 0.0, cross_entropy_loss = 4.954282, global_step = 17150, loss = 5.003105\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 17150: /tmp/tf_bow_sst_20181203-1844/model.ckpt-17150\n"
     ]
    }
   ],
   "source": [
    "# Training params, just used in this cell for the input_fn-s\n",
    "train_params = dict(batch_size=125, total_epochs=10, eval_every=2)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Construct and train the model, saving checkpoints to the directory above.\n",
    "# Input function for training set batches\n",
    "# Do 'eval_every' epochs at once, followed by evaluating on the dev set.\n",
    "# NOTE: use patch_numpy_io.numpy_input_fn instead of tf.estimator.inputs.numpy_input_fn\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": train_x, \"ns\": train_n}, y=train_y,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n",
    "\n",
    "# Input function for dev set batches. As above, but:\n",
    "# - Don't randomize order\n",
    "# - Iterate exactly once (one epoch)\n",
    "dev_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": dev_x, \"ns\": dev_n}, y=dev_y,\n",
    "                    batch_size=125, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on dev\n",
    "    model.train(input_fn=train_input_fn)\n",
    "    eval_metrics = model.evaluate(input_fn=dev_input_fn, name=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-03-18:47:15\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181203-1844/model.ckpt-17150\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-03-18:47:16\n",
      "INFO:tensorflow:Saving dict for global step 17150: accuracy = 1.0, cross_entropy_loss = 0.008333357, global_step = 17150, loss = 0.057105113\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 17150: /tmp/tf_bow_sst_20181203-1844/model.ckpt-17150\n",
      "Accuracy on test set: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 1.0,\n",
       " 'cross_entropy_loss': 0.008333357,\n",
       " 'loss': 0.057105113,\n",
       " 'global_step': 17150}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# test_input_fn = None  # replace with an input_fn, similar to dev_input_fn\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": test_x, \"ns\": test_n}, y=test_y,\n",
    "                    batch_size=125, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn = test_input_fn)  # replace with result of model.evaluate(...)\n",
    "\n",
    "#### END(YOUR CODE) ####\n",
    "print(\"Accuracy on test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20181203-1844/model.ckpt-17150\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Accuracy on test set: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predictions = list(model.predict(test_input_fn))  # list of dicts\n",
    "y_pred = [p['max'] for p in predictions]\n",
    "acc = accuracy_score(y_pred, test_y)\n",
    "\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "16608\n",
      "16608\n",
      "[{'proba': array([0.99221593, 0.0077841 ], dtype=float32), 'max': 0}, {'proba': array([0.99614114, 0.0038589 ], dtype=float32), 'max': 0}]\n"
     ]
    }
   ],
   "source": [
    "# predicted labels\n",
    "print(y_pred[0:50])\n",
    "print(len(y_pred))\n",
    "print(len(test_y))\n",
    "print(predictions[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felixtsui/anaconda3/lib/python3.6/site-packages/sklearn/metrics/ranking.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "    \n",
    "\n",
    "average_precision = average_precision_score(y_pred,test_y)\n",
    "\n",
    "print('Average precision score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
